---
title: "強化学習：報酬から学ぶ"
coverImage: "/assets/blog/preview/cover.jpg"
date: "2026-02-01T13:00:00.000Z"
ogImage:
  url: "/assets/blog/preview/cover.jpg"
---

強化学習（Reinforcement Learning）は、**環境との相互作用**を通じて、報酬を最大化するような行動を学習する機械学習の手法です。教師あり・教師なし学習とは異なり、「正解ラベル」や「クラスタ」ではなく、**報酬（とペナルティ）** を手がかりに学習します。ゲームAI、ロボット制御、推薦・広告などに広く応用されています。この記事では、強化学習の基本に加え、**バンディットアルゴリズム・マルコフ決定過程（MDP）・価値関数・方策勾配**の4つを詳しく解説し、そのうえでQ学習・DQN・Actor-Critic・PPOなどの代表的なアルゴリズムと応用に触れます。

### この記事でわかること

- 強化学習の考え方（エージェント・環境・状態・行動・報酬・方策）と、教師あり・教師なしとの違い
- バンディット問題とε-greedy・UCB・トンプソンサンプリングの概要と応用
- MDPの形式的定義（状態・行動・遷移・報酬・割引率）とマルコフ性
- 価値関数（\( V^\pi \)、\( Q^\pi \)）とベルマン方程式・最適方策の関係
- 方策勾配定理とREINFORCE・ベースライン・アドバンテージの考え方

---

## 強化学習とは

強化学習では、**エージェント**（学習し、意思決定を行う主体）が**環境**（エージェントの外側の世界）に対して**行動**を選択し、その結果として**状態**が変化し、**報酬**（またはペナルティ）を受け取ります。正解の行動は教えられませんが、「どの行動がどれだけ良かったか」が報酬として返ってくるため、エージェントは**長期的に得られる報酬を最大化するような振る舞い**を学習します。その振る舞いのルールを**方策（ポリシー）** といい、学習の目的は「良い方策を見つけること」です。

### 教師あり・教師なし学習との違い

| 項目 | 教師あり学習 | 教師なし学習 | 強化学習 |
|------|--------------|--------------|----------|
| 学習の手がかり | 正解ラベル | データの構造 | 報酬（フィードバック） |
| データ | 入力と正解のペア | 入力のみ | 状態・行動・報酬の系列 |
| 目的 | 予測・分類 | 構造の発見 | 報酬の最大化 |
| 例 | 画像分類、売上予測 | クラスタリング、次元削減 | ゲームAI、ロボット制御 |

### 特徴

- **試行錯誤**: 正解が与えられない代わりに、行動の結果として報酬が得られる。
- **遅延報酬**: 報酬が直後ではなく、何ステップか後に得られることがある（信用割り当て問題）。
- **探索と利用のトレードオフ**: 未知の行動を試す（探索）と、今わかっている良い行動を選ぶ（利用）のバランスを取る必要がある。

## 強化学習の基本用語

### エージェント（Agent）

**エージェント**は、学習し、意思決定を行う主体です。ロボット、ゲームのプレイヤー、推薦システムなどがエージェントに相当します。エージェントは「どの行動を選ぶか」を決め、その結果を観測して学習します。

### 環境（Environment）

**環境**は、エージェントの外側の世界です。エージェントが行動を取ると、環境の状態が変化し、その結果として報酬が返されます。シミュレーター、実世界、ゲームエンジンなどが環境になります。強化学習では、環境が「状態・行動・報酬」のルールに従って動くと仮定し、そのルールの下で最良の行動の選び方（方策）を学習します。

### 状態（State）\( s \)

環境が「今どうなっているか」を表す情報です。ゲームなら盤面やスコア、ロボットならセンサー値や位置などです。

### 行動（Action）\( a \)

エージェントが選択できる操作の集合です。離散（上・下・左・右など）の場合も、連続（速度、角度など）の場合もあります。

### 報酬（Reward）\( r \)

行動の結果として環境から与えられるスカラー値です。ゴールに近づけば正の報酬、失敗すれば負の報酬（ペナルティ）など、問題に応じて設計します。

### 方策（Policy）\( \pi \)

**方策**は、状態から行動への写像です。「この状態ではこの行動を選ぶ」というルールのことで、学習の目的は**報酬を最大化する方策**（最適方策）を見つけることです。方策が決まれば、エージェントはどの状態でも「どの行動を選ぶか」が決まるため、方策の良し悪しがそのまま長期的な報酬の多さに対応します。

- **決定論的方策**: \( a = \pi(s) \) のように、状態に対して行動が一意に決まる。
- **確率的方策**: \( \pi(a|s) \) のように、状態が与えられたときの行動の確率分布を表す。

### 価値関数（Value Function）

- **状態価値関数 \( V^\pi(s) \)**: 状態 \( s \) から方策 \( \pi \) に従って行動したときの、**期待される累積報酬（割引付き）**。
- **行動価値関数 \( Q^\pi(s, a) \)**: 状態 \( s \) で行動 \( a \) を選び、その後方策 \( \pi \) に従ったときの、**期待される累積報酬**。Q学習などで用いられる。詳細は後述の「価値関数」を参照。

### 割引率（Discount Factor）\( \gamma \)

将来の報酬をどれだけ重視するかを表す \( 0 \leq \gamma \leq 1 \) のパラメータです。\( \gamma \) が1に近いと将来の報酬を重視し、0に近いと目の前の報酬を重視します。短期で終わるタスクでは \( \gamma \) を小さく、長期の計画が重要なタスクでは \( \gamma \) を1に近くすることが多いです。

---

## バンディットアルゴリズム（Bandit Algorithms）

強化学習のうち、**状態が実質1つ**（または状態を考えない）で、**探索と利用のトレードオフ**だけが効く最もシンプルな設定が**バンディット問題**です。スロットマシン（アーム）を何本も並べ、各アームを引くと確率的に報酬が得られ、どのアームが期待報酬が高いかは未知という状況をモデルにします。ここで扱うアルゴリズムは、のちにMDPでの「どの行動を試すか」の探索方策にも応用されます。

### 多腕バンディット（Multi-Armed Bandit）

- **設定**: \( K \) 本のアーム（腕）があり、時刻 \( t \) でアーム \( a_t \in \{1,\ldots,K\} \) を1本選ぶ。選んだアームから**報酬 \( r_t \)** が得られ、報酬の分布はアームごとに異なりますが、どのアームが期待報酬が高いかは**未知**です。目的は**累積報酬**を最大化することです。理論的には、**リグレット**（「最良のアームを常に引いた場合に得られたであろう報酬」との差）を最小化する問題としても定式化され、リグレットが小さいほど「最良に近い選択」ができていることを意味します。
- **探索と利用**: 今わかっている中で期待報酬が高いアームを引くのが**利用**、まだ試していないアームや不確実性が大きいアームを試すのが**探索**です。探索を怠ると、最良でないアームに固執してしまう可能性があります。一方、探索しすぎると、明らかに悪いアームも同じように試して報酬を捨てることになります。そのため、**探索と利用のバランス**をどう取るかが、バンディットアルゴリズムの核心です。

### ε-greedy（イプシロン・グリーディ）

- **方策**: 確率 \( \varepsilon \) で**ランダムにアームを選ぶ**（探索）、確率 \( 1 - \varepsilon \) で**これまでの標本平均が最大のアームを選ぶ**（利用）。
- **メリット**: 実装が簡単で直感的。\( \varepsilon \) を時間とともに減らす（例：\( \varepsilon_t = 1/t \)）と、漸近的には最良アームに収束しやすい。
- **デメリット**: 探索時にランダムなので、明らかに悪いアームも同じ確率で選んでしまう。リグレットの理論的な保証は弱い。

### UCB（Upper Confidence Bound）

- **考え方**: 各アームの**期待報酬の上界**（推定値＋不確実性のボーナス）を計算し、**上界が最大のアーム**を選ぶ。不確実性が大きいアームほど「試す価値がある」とみなす。
- **UCB1**: アーム \( a \) を \( n_a \) 回引いたとき、\( \bar{r}_a + \sqrt{\frac{2 \ln t}{n_a}} \) を上界として使う。\( \bar{r}_a \) は標本平均、\( t \) は総試行数。この上界が最大のアームを選ぶ。
- **メリット**: 探索が「不確実性が大きいところ」に集中し、リグレットの理論的上界が得られる（対数オーダー）。
- **デメリット**: ハイパーパラメータ（上界の係数）の調整が必要な場合がある。

### トンプソンサンプリング（Thompson Sampling）

- **考え方**: 各アームの報酬分布を**ベイズ的に**モデル化し、事後分布から**サンプル**を1つ取り、そのサンプルで「期待報酬が最大のアーム」を選ぶ。確率的に探索と利用が自然に混ざる。
- **例（ベルヌーイバンディット）**: 各アームの報酬が0か1で、真の確率 \( p_a \) が未知のとき、事前にベータ分布を置き、観測ごとに事後を更新する。各ステップで各 \( a \) の事後から \( p_a \) を1つサンプルし、\( \arg\max_a p_a \) のアームを選ぶ。
- **メリット**: 多くの設定で経験的・理論的に良い性能。実装も比較的容易。
- **デメリット**: 事後分布の更新とサンプリングのコストがかかる（共役事前分布を使えば軽い）。

### バンディットの応用

- **Web広告**: 複数の広告（アーム）のうち、クリック率やコンバージョンが高いものを選ぶ。ユーザーごとに状態を考えると**文脈付きバンディット（Contextual Bandit）** になる。
- **推薦・A/Bテスト**: 複数の推薦方策やUIのうち、どのが良いか試しながら選ぶ。
- **強化学習の探索**: 多腕バンディットのアルゴリズムは、MDPでも「どの行動を試すか」の探索方策として応用される（例：UCBをQ値に組み込む）。

---

## マルコフ決定過程（MDP: Markov Decision Process）

強化学習では、環境を**マルコフ決定過程（MDP）**で定式化することが多いです。MDPは**状態・行動・遷移・報酬・割引率**で定義され、エージェントの目的は**期待累積報酬（割引付き）を最大化する方策**を見つけることです。

### マルコフ性

**次の状態と報酬は、現在の状態と行動だけで決まり、過去の履歴には依存しない**という仮定です。つまり \( P(s_{t+1}, r_t \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots) = P(s_{t+1}, r_t \mid s_t, a_t) \) とみなします。これにより、現在の状態 \( s_t \) と行動 \( a_t \) だけが「今後の運命」を決めるため、価値関数や方策を状態（と行動）の関数として扱えます。

### MDPの形式的定義

- **状態空間 \( \mathcal{S} \)**: 取り得る状態の集合。離散の場合も連続の場合もある。
- **行動空間 \( \mathcal{A} \)**: 各状態で取り得る行動の集合。状態によって使える行動が異なる場合は \( \mathcal{A}(s) \) と書く。
- **遷移確率 \( P(s' \mid s, a) \)**: 状態 \( s \) で行動 \( a \) を取ったときに、次状態が \( s' \) になる確率（確定的な環境なら1つの \( s' \) に集中）。
- **報酬関数 \( R(s, a, s') \) または \( r(s, a) \)**: 状態 \( s \) で行動 \( a \) を取り、次状態 \( s' \) に移ったときに得る報酬。期待報酬 \( r(s, a) = \mathbb{E}[R \mid s, a] \) だけを考えることも多い。
- **割引率 \( \gamma \in [0, 1] \)**: 将来の報酬をどれだけ割り引くか。

これらをまとめて MDP は \( (\mathcal{S}, \mathcal{A}, P, R, \gamma) \) で表されます。

### エピソードと累積報酬

エージェントは時刻 \( t = 0, 1, 2, \ldots \) で状態 \( s_t \) を観測し、行動 \( a_t \) を選び、報酬 \( r_t \) を受け取り、次状態 \( s_{t+1} \) に移ります。**割引付き累積報酬（リターン）** は \( G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots \) で、方策 \( \pi \) の良さは**期待リターン \( \mathbb{E}^\pi[G_t \mid s_t = s] \)**（および \( a_t = a \) を固定した場合）で評価します。

### MDPとバンディット

バンディットは「状態が1つ（または無視できる）」MDPとみなせます。MDPでは状態が変わるため、**どの状態でどの行動が良いか**を学習する必要があり、バンディットより一般的で難しい問題になります。その分、価値関数や方策の定式化（次の節以降）が重要になります。

---

## 価値関数（Value Function）

MDPで方策の良さを数値で表すために、**期待割引累積報酬**を状態（および状態と行動のペア）の関数として定義したものが価値関数です。ベルマン方程式により、価値関数は「即時報酬＋次の状態の価値」で再帰的に書け、これが動的計画法やTD学習・Q学習の基礎になります。

**価値関数**は、ある方策 \( \pi \) に従ったときに、状態（および状態と行動のペア）から得られる**期待割引累積報酬**を表します。「この状態（と行動）から先、どれだけの報酬が期待できるか」を数値化したもので、方策の良さを比較するための指標になります。**ベルマン方程式**により、価値関数は「今得る報酬の期待値＋割引した次の状態の価値の期待値」という形で**再帰的に**書けます。つまり、長い将来を一度に計算する代わりに、「1ステップ先の価値」を使って今の価値を表せるため、動的計画法やTD学習・Q学習で逐次的に推定・更新できるようになり、最適方策を求める基礎になります。

### 状態価値関数 \( V^\pi(s) \)

方策 \( \pi \) に従うとき、**状態 \( s \) から得られる期待リターン**です。

\[
V^\pi(s) = \mathbb{E}_\pi\bigl[ G_t \mid s_t = s \bigr] = \mathbb{E}_\pi\bigl[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots \mid s_t = s \bigr]
\]

「状態 \( s \) にいるとき、これから方策 \( \pi \) で行動し続けたら、平均してどれだけの報酬が得られるか」を表します。

### 行動価値関数 \( Q^\pi(s, a) \)

方策 \( \pi \) に従うとき、**状態 \( s \) で行動 \( a \) を1回取り、その後 \( \pi \) に従う**場合の期待リターンです。

\[
Q^\pi(s, a) = \mathbb{E}_\pi\bigl[ G_t \mid s_t = s, a_t = a \bigr] = \mathbb{E}_\pi\bigl[ r_t + \gamma r_{t+1} + \cdots \mid s_t = s, a_t = a \bigr]
\]

\( V^\pi(s) \) と \( Q^\pi(s, a) \) の関係は、\( V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s, a) \)（方策 \( \pi \) で行動を平均）です。

### ベルマン方程式（Bellman Equation）

価値関数は、**即時報酬**と**次の状態の価値**で再帰的に書けます。

- **状態価値のベルマン方程式**:
\[
V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) \bigl[ r(s,a,s') + \gamma V^\pi(s') \bigr]
\]

- **行動価値のベルマン方程式**:
\[
Q^\pi(s,a) = \sum_{s'} P(s'|s,a) \bigl[ r(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q^\pi(s',a') \bigr]
\]

「今の価値 = 今得る報酬の期待値 + 割引した次の状態の価値の期待値」という形です。この再帰関係があるおかげで、遷移確率と報酬が分かっていれば**動的計画法（DP）** で価値関数を計算でき、分からなくても**TD学習**（ Temporal Difference Learning：実際に得た報酬と次の状態の価値の推定値から、今の価値を更新する）でサンプルから学習できます。

### 最適価値関数と最適方策

**最適状態価値関数** \( V^*(s) = \max_\pi V^\pi(s) \)、**最適行動価値関数** \( Q^*(s,a) = \max_\pi Q^\pi(s,a) \) は、どの方策でも達成し得る最大の期待リターンを表します。**最適方策 \( \pi^* \)** は \( V^{\pi^*}(s) = V^*(s) \) を満たす方策で、各状態で \( Q^*(s,a) \) を最大にする行動を選べばよい（貪欲方策）ことが知られています。

**最適ベルマン方程式**（ベルマン最適性方程式）は次の形です。

\[
Q^*(s,a) = \sum_{s'} P(s'|s,a) \bigl[ r(s,a,s') + \gamma \max_{a'} Q^*(s',a') \bigr]
\]

Q学習はこの式に基づき、\( Q \) を \( \max_{a'} Q(s',a') \) を目標として更新します。価値関数を経由せずに方策を直接改善する**方策勾配**とは対照的に、Q学習は**価値関数（Q）を学習し、その最大を取る行動**を方策とするアプローチです。

---

## 方策勾配（Policy Gradient）

価値関数を推定してから貪欲方策を取るのではなく、**方策そのものをパラメータで表現し、期待リターンの勾配で直接更新する**手法が方策勾配法です。連続行動空間や確率的方策での探索に適しており、Actor-Criticでは価値関数（クリティック）で勾配の分散を抑えつつ用いられます。

**方策勾配法**は、方策 \( \pi \) をパラメータ \( \theta \) で表し（\( \pi_\theta \)）、**期待リターンを \( \theta \) で微分した勾配**に沿って \( \theta \) を更新する手法です。Q学習のように「まず価値関数を学習し、その最大を取る行動を方策とする」のではなく、**方策そのものを直接**パラメータの更新で改善します。連続行動空間（例：ロボットの関節角度）では、行動が無限にありQの最大化が難しいため、方策を直接パラメータ化する方策勾配法が自然に使われます。

### 方策勾配定理（Policy Gradient Theorem）

目的を \( J(\theta) = \mathbb{E}_{\pi_\theta}[G_0] \)（初期状態からの期待リターン）とすると、その勾配は次の形で書けます（方策勾配定理）。

\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}\bigl[ \nabla_\theta \log \pi_\theta(a|s) \, G_t \bigr]
\]

つまり、**各ステップで \( \log \pi_\theta(a|s) \) の勾配にリターン \( G_t \) をかけたもの**の期待値を取ると、目的関数の勾配になる。実際には期待値の代わりに**サンプル（1エピソードや複数ステップ）** で近似し、\( \theta \) を勾配 ascend で更新します。

### REINFORCE（モンテカルロ方策勾配）

**REINFORCE**は、エピソード終了まで得た**実際のリターン \( G_t \)** を使って勾配を推定する方法です。

1. 方策 \( \pi_\theta \) に従ってエピソードを1本生成する。
2. 各ステップ \( t \) で \( G_t \) を計算する（終端から遡る）。
3. \( \nabla_\theta J \approx \sum_t \nabla_\theta \log \pi_\theta(a_t|s_t) \, G_t \) で勾配を推定し、\( \theta \leftarrow \theta + \alpha \nabla_\theta J \) で更新する。

**メリット**: 実装が簡単で、方策勾配の基礎になる。**デメリット**: \( G_t \) が1本のエピソードだけなので**分散が大きい**。学習が不安定になりやすい。

### ベースラインとアドバンテージ

勾配の分散を減らすため、**ベースライン \( b(s_t) \)** を引いた量を使います。

\[
\nabla_\theta J \propto \mathbb{E}_{\pi_\theta}\bigl[ \nabla_\theta \log \pi_\theta(a|s) \, \bigl( G_t - b(s) \bigr) \bigr]
\]

\( b(s) \) が \( G_t \) に依存しなければ、期待値は変わらない（不偏）が、分散は減らせる。\( b(s) = V^\pi(s) \) とすると \( G_t - V^\pi(s_t) \) は**アドバンテージ**の推定になり、**アドバンテージアクター・クリティック（A2C）** などでは、クリティック（価値関数）で \( V \) を近似し、\( A_t \approx G_t - \hat{V}(s_t) \) や TD誤差で勾配を計算します。

### 方策勾配のメリット・デメリット

- **メリット**: **連続行動空間**にそのまま使える（Q学習は離散行動が自然）。**確率的方策**で探索しやすい。方策を直接パラメータ化できる（例：ガウス方策、Softmax）。
- **デメリット**: 勾配の**分散が大きい**ため、ベースラインやクリティックなしだと学習が不安定。オンラインで1エピソードずつだとサンプル効率が悪い場合がある。

---

## 代表的なアルゴリズム

ここまでに解説した**価値関数**と**方策勾配**を基礎に、実務でよく使われるアルゴリズムを簡潔にまとめます。価値ベース（Qを学習して行動を決める）と方策ベース（方策を直接更新する）およびその組み合わせ（Actor-Critic）があります。

### 価値ベース：Q学習（Q-Learning）

**行動価値関数 \( Q(s, a) \) を学習**し、各状態で \( Q \) が最大となる行動を選ぶ方策にします。**オフポリシー**（学習する方策と行動する方策が異なってよい）で、テーブル形式のQ値の更新が基本です。

- **更新式**: \( Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)] \)
- **メリット**: シンプルで理解しやすい、離散状態・離散行動の小規模問題に有効。
- **デメリット**: 状態・行動が連続や大規模だと、Qをテーブルで持てないため、関数近似（DQNなど）が必要。

### 深層強化学習：DQN（Deep Q-Network）

Q学習の \( Q(s, a) \) を**ニューラルネットワーク**で近似した手法です。経験再生（Replay Buffer）とターゲットネットワークで安定化を図ります。

- **経験再生**: 過去の遷移 \( (s, a, r, s') \) をバッファに貯め、そこからランダムにサンプリングして学習する。
- **ターゲットネットワーク**: 目標値の計算に使うネットワークを定期的に更新し、目標が変わりすぎるのを防ぐ。
- **応用**: ゲームAI（Atariなど）、推薦、広告配信など。

### 方策ベース：方策勾配法（Policy Gradient）

方策 \( \pi_\theta(a|s) \) をパラメータ \( \theta \) で表現し、方策勾配定理に基づいて \( \nabla_\theta J(\theta) \) を推定し、勾配 ascend で \( \theta \) を更新します。詳細は前節の**方策勾配**を参照。REINFORCE、ベースライン、アドバンテージ（Actor-Critic）などで分散を抑えつつ学習します。

### アクター・クリティック（Actor-Critic）

**方策（アクター）** と**価値関数（クリティック）** の両方を学習する枠組みです。アクターが行動を選び、クリティックがその評価（TD誤差など）を計算し、その信号でアクターを更新します。

- **メリット**: 方策勾配の分散を減らし、学習を安定させやすい。
- **例**: A2C、A3C、PPO、SAC など、多くの現代アルゴリズムがこの枠組みに含まれる。

### PPO（Proximal Policy Optimization）

方策を更新する際に、**更新幅を制限**することで学習の安定性を高めた手法です。実装が比較的容易で、多くのタスクで良い性能が出ることから広く使われています。

- **アイデア**: 方策の変化が大きくなりすぎないように、クリップやペナルティを入れて更新する。
- **応用**: ロボット制御、ゲーム、シミュレーションなど。

## 強化学習の応用

- **ゲームAI**: 囲碁（AlphaGo）、将棋、Atariゲーム、StarCraftなど。
- **ロボット制御**: 歩行、把持、マニピュレーション。
- **自律走行**: 経路計画、車線維持、合流・追従。
- **推薦・広告**: ユーザーとの長期インタラクションを報酬とみなし、クリックや購入を最大化する方策を学習。
- **在庫・リソース管理**: 在庫補充、サーバーリソース割り当てなど、逐次意思決定問題。

## 強化学習の課題と実践のポイント

### 課題

- **サンプル効率**: 多くのアルゴリズムは、学習に大量の試行（環境との相互作用）を必要とする。
- **報酬の設計**: 報酬の形が学習の結果に大きく影響する。意図しない振る舞いを報酬してしまうと、望ましくない方策が学習される（報酬ハッキング）。
- **探索と利用**: 探索が足りないと局所解に陥り、探索しすぎると収束が遅い。
- **シミュレーションと実世界のギャップ**: シミュレーションで学習した方策を実機に移すと、モデル化の誤差で性能が落ちることがある（Sim-to-Real）。

### 実践のポイント

1. **問題のMDP化**: 状態・行動・報酬を明確に定義する。
2. **報酬設計**: ゴールに沿った報酬と、望ましくない振る舞いを防ぐペナルティのバランスを取る。
3. **環境**: 可能ならシミュレーターで大量に試行し、必要に応じて実環境でファインチューニングする。
4. **アルゴリズム選択**: 離散/連続、状態・行動の規模、サンプル効率の要求に応じて、Q学習、DQN、PPO、SACなどを選ぶ。

## まとめ

本記事の要点をまとめます。

- 強化学習は、**環境との相互作用**と**報酬**に基づいて、**報酬を最大化する方策**を学習する枠組みです。正解の行動は教えられませんが、報酬が「どの行動が良かったか」の手がかりになります。
- **バンディットアルゴリズム**は、状態を考えない探索・利用のモデルです。ε-greedy、UCB、トンプソンサンプリングなどがあり、Web広告・推薦・A/Bテストに応用されます。
- **マルコフ決定過程（MDP）** は、状態・行動・遷移確率・報酬・割引率で定義され、マルコフ性（次の状態と報酬が「今の状態と行動」だけで決まる）の下で、期待累積報酬を最大化する方策を求めます。
- **価値関数**（\( V^\pi \)、\( Q^\pi \)）は期待割引累積報酬を表し、**ベルマン方程式**（再帰関係）と**最適ベルマン方程式**が、動的計画法・TD学習・Q学習の基礎になります。
- **方策勾配**は、方策を直接パラメータ化し、期待リターンの勾配で更新する手法です。REINFORCE、ベースライン、アドバンテージ（Actor-Critic）で勾配の分散を抑えつつ学習します。
- **Q学習、DQN、Actor-Critic、PPO**など、価値ベース・方策勾配・Actor-Criticの各種アルゴリズムが、ゲームAI・ロボット・推薦などに応用されます。報酬設計・サンプル効率・シミュレーションと実世界のギャップ（Sim-to-Real）などが実務上の課題です。

次の記事では、**モデルの選択・評価**（データの分割・交差検証、回帰・分類の評価指標、ROC曲線とAUC、AIC・BICと交差検証によるモデル選択）について解説します。
