---
title: "ディープラーニングの要素技術"
coverImage: "/assets/blog/preview/cover.jpg"
date: "2026-02-03T11:00:00.000Z"
ogImage:
  url: "/assets/blog/preview/cover.jpg"
---

ディープラーニングを実践するうえで、**畳み込みニューラルネットワーク（CNN）**・**リカレントニューラルネットワーク（RNN）**・**Transformer**・**正則化・正規化**・**オプティマイザ**・**フレームワーク**といった要素技術が欠かせません。それぞれが「どのデータに適しているか」「なぜその形なのか」を理解しておくと、タスクに合わせたモデル設計や既存手法の読み解きがしやすくなります。この記事では、これらの**要素技術の役割・仕組み・代表例**を、直感的に理解できるように詳しく解説します。

### この記事でわかること

- CNN・RNN・Transformerの違いと、画像・時系列・自然言語への適性
- 正則化（Dropout、Weight Decay）と正規化（Batch Normalization）の目的と効果
- 代表的なオプティマイザ（SGD、Adam）と学習率スケジューリングの考え方
- 主要フレームワーク（PyTorch、TensorFlow）の役割と開発の流れ

---

## 畳み込みニューラルネットワーク（CNN）

**畳み込みニューラルネットワーク（Convolutional Neural Network, CNN）** は、**画像や時系列のような「並び」のデータ**に適したネットワークです。「並びのデータ」とは、画像なら縦・横に並んだピクセル、時系列なら時間順に並んだ値のように、**隣同士の位置に意味がある**データのことです。CNN は「隣のピクセル同士の関係」や「近い時刻の関係」を活かすように設計されているため、画像や時系列で特に力を発揮します。**局所的なパターン**を抽出し、**位置が少しずれても同じパターンとして認識**できるようにする**畳み込み（Convolution）** と、**空間・時間方向の解像度を落として情報を集約**する**プーリング（Pooling）** を組み合わせることで、パラメータ数を抑えつつ、画像認識・物体検出・セグメンテーションなどで高い性能を発揮します。

### 畳み込み（Convolution）の考え方

**畳み込み**では、**カーネル（フィルタ）** という小さな重みの塊を、入力（画像なら縦・横）に沿って**スライドさせ**、各位置で「カーネルと入力の対応する部分」の**内積**を計算します。これにより、

- **局所性**: 各出力は「入力のごく近傍」の情報だけに依存する。画像なら「この周辺のピクセル」だけを見る。
- **共有重み**: 同じカーネルを画像全体で使い回すため、**どこにパターンがあっても同じ重みで検出**できる（平行移動不変性に近い性質）。その結果、**パラメータ数が少なく**、過学習を抑えやすく、学習も効率的になる。

という性質が得られます。「共有重み」とは、画像の左上でも右下でも**同じ重みのカーネル**を使うということです。そのため「縦のエッジ」のようなパターンは、画像のどこにあっても同じカーネルで検出でき、パラメータ数を抑えつつ「位置がずれても同じパターンとして認識する」ことができます。カーネルを複数種類用意すると、**エッジ・テクスチャ・色のパッチ**など、異なるパターンを同時に検出できます。出力は**特徴マップ**と呼ばれ、縦・横・チャネル（フィルタ数）の3次元（または時系列なら1次元＋チャネル）になります。

### プーリング（Pooling）

**プーリング**は、**局所領域内の値を1つにまとめる**操作です。**最大プーリング（Max Pooling）** では、例えば 2×2 の領域ごとに**最大値**を取ります。これにより、

- **位置のずれに対する頑健性**: パターンが少しずれても、その領域内の最大値が取られるため、出力が変わりにくい。
- **解像度の低下**: 縦・横のサイズが減り、**計算量とパラメータが減る**一方、**抽象度が上がる**（細かい位置情報は捨て、「このあたりに反応があった」という情報が残る）。

平均プーリング（領域内の平均を取る）も使われますが、画像では最大プーリングが多く用いられます。

### CNNの典型的な構成

画像認識用のCNNでは、**「畳み込み → 活性化（ReLU）→ プーリング」** のブロックを何度か重ねたあと、**全結合層（FC）** で1次元にまとめ、**ソフトマックス**でクラス確率を出力する構成が典型的でした（AlexNet、VGGなど）。その後、**ResNet** では**残差接続（スキップ接続）** により、勾配が層をまたいで流れやすくなり、**非常に深いネットワーク**が安定して学習できるようになりました。**残差接続**とは、ある層の入力をその層の出力に**足し戻す** \( \boldsymbol{x}_{l+1} = \boldsymbol{x}_l + F(\boldsymbol{x}_l) \) ことで、\( F \) が小さな補正を学ぶ形にし、深さによる勾配消失を緩和する仕組みです。

---

## リカレントニューラルネットワーク（RNN）とLSTM

**リカレントニューラルネットワーク（Recurrent Neural Network, RNN）** は、**時系列やテキストのような「順序付きデータ」** を扱うためのネットワークです。「順序付き」とは、**並ぶ順番に意味がある**データのことです。文章なら「私 / は / 学生 / です」の順序が変わると意味が変わりますし、時系列なら「過去→現在→未来」の流れが重要です。RNN は「1つ前の時刻の状態」を覚えながら次の出力を計算するため、**文脈**や**過去の流れ**を考慮した予測ができます。**同じ重み**を時刻（またはトークン）ごとに繰り返し使い、**隠れ状態（内部メモリ）** を更新しながら次の出力を計算するため、「過去の情報を踏まえた文脈」を扱えます。

### RNNの基本

各時刻 \( t \) で、**入力** \( \boldsymbol{x}_t \) と前の時刻の隠れ状態 \( \boldsymbol{h}_{t-1} \) を受け取り、**新しい隠れ状態** \( \boldsymbol{h}_t \) と**出力** \( \boldsymbol{y}_t \) を計算します。式で書くと \( \boldsymbol{h}_t = \sigma(W_{hh} \boldsymbol{h}_{t-1} + W_{xh} \boldsymbol{x}_t + \boldsymbol{b}) \) のようになり、**重み** \( W_{hh}, W_{xh} )は全時刻で共有 されます。このため、理論上は **任意長の系列**を扱えますが、実際には**長い系列で勾配が消失または爆発**しやすく、長い依存関係を学習するのが難しいという課題がありました。

### LSTM（Long Short-Term Memory）

**LSTM** は、RNNの一種で、**セル状態（長期メモリ）** と**ゲート（入力・忘却・出力）** を導入し、**長期の依存関係**を学習しやすくしたものです。

- **忘却ゲート**: セル状態の「どの部分を忘れるか」を決める。
- **入力ゲート**: 新しい情報の「どの部分をセルに足すか」を決める。
- **出力ゲート**: セル状態の「どの部分を隠れ状態として出力するか」を決める。

これにより、勾配がセル状態を経由して比較的安定して流れ、**長い文や長い時系列**でも文脈を保持しやすくなりました。機械翻訳・音声認識・時系列予測などで広く使われましたが、**並列計算がしにくい**ことと、**非常に長い文脈**では依然として限界があることから、自然言語では**Transformer**に置き換わっていきました。

---

## TransformerとAttention

**Transformer** は、**Attention（注意）機構** を中心にしたモデルで、**並列計算**が可能であり、**長い系列の依存関係**を直接扱えるため、自然言語処理（NLP）をはじめ、画像・音声・マルチモーダルでも標準的なアーキテクチャになっています。「注意」とは、**今見ている位置が、系列の他のどの位置の情報をどれだけ参照するか**を、重み（Attention の重み）で決める仕組みです。例えば「その」という単語が「前の文のどの名詞を指すか」を、文脈の各単語との類似度で決めるようなイメージです。RNN は1つずつ順に処理するため並列化しにくいですが、Transformer は全位置を一度に扱えるため、GPU で高速に学習できます。

### Attention（注意）の考え方

**Attention** では、**「今見ている位置」が「系列の他の位置」の情報を、重要度に応じて重み付きで参照**します。**Query（問い合わせ）** ・**Key（鍵）** ・**Value（値）** の3つを用意し、Query と各 Key の類似度（内積など）を計算して**重み（Attention の重み）** とし、その重みで Value を加重和します。これにより、「この単語はあの単語に注目する」といった**柔軟な参照**が学習され、**長距離依存**も1層で扱えます。RNNのように順番に処理する必要がないため、**全位置を並列**で計算でき、GPUでの学習が非常に高速です。

### Transformerの構成

Transformerでは、**Encoder** と **Decoder** からなる構成が基本です（Encoder-only、Decoder-only、Encoder-Decoder などバリエーションあり）。

- **Encoder**: 入力系列の各位置を、**Self-Attention**（同じ系列内で Query/Key/Value を取る）と**順伝播層（FFN）** で変換し、**位置ごとの表現**を出力する。**位置エンコーディング**で「何番目のトークンか」の情報を加える。
- **Decoder**: 自己回帰的に**次のトークンを予測**する。Decoder 内では、**Masked Self-Attention**（未来の位置を見ない）と、**Encoder の出力に対する Cross-Attention** により、入力系列を参照しつつ出力を生成する。

**Multi-Head Attention** では、Attention を複数並列に計算し、結果を結合します。これにより、**異なる種類の関係**（文法、共参照、長距離など）を同時に学習しやすくなります。**Layer Normalization** と **残差接続** が各サブレイヤーの前後に置かれ、学習の安定化と深さの確保に役立っています。

---

## 正則化と正規化

**過学習**（訓練データに合わせすぎて、未知データでの性能が落ちる）を防ぐため、**正則化**と**正規化**が広く使われます。過学習とは、**訓練データの細かいノイズまで覚えてしまい、本番で見る新しいデータでは誤差が大きくなる**状態です。正則化は「重みを大きくしすぎない」「一部のユニットをランダムに無効にする」などで**モデルをシンプルに保ち**、正規化は「層の出力の分布を整える」ことで**学習の揺れを抑え**、どちらも汎化（未知データでの性能）を良くするために使われます。

### 正則化

- **Weight Decay（L2正則化）**: 損失に **\( \lambda \sum \theta^2 \)** を加え、重みを全体的に小さくする。オプティマイザの設定で「重みの減衰」として組み込まれることが多い。
- **Dropout**: 学習時に**ランダムに一部のユニットの出力を0**にする。これにより、特定のユニットに依存しすぎず、**アンサンブル的な効果**で汎化が期待される。推論時は Dropout をオフにし、全ユニットを使う（または出力をスケールする）。

これらにより、モデルが訓練データのノイズまで覚えにくくなり、**検証・テストデータでの性能**が改善しやすくなります。

### 正規化（Batch Normalization / Layer Normalization）

**正規化**は、**中間層の出力の分布を整える**ことで、学習を安定させ、深いネットワークや大きな学習率でも訓練しやすくする技術です。

- **Batch Normalization（BN）**: **ミニバッチ内**で、各チャネルごとに平均0・分散1になるように正規化し、**スケールとシフト**のパラメータを学習する。CNNでよく使われ、学習の収束が速くなり、学習率を大きく取りやすい。推論時は、訓練時に蓄えた移動平均の平均・分散を使う。
- **Layer Normalization（LN）**: **各サンプル・各層**で、その層の全ユニットについて平均0・分散1に正規化する。バッチサイズに依存しにくく、**RNNやTransformer** で標準的に使われる。

正規化により、**勾配のスケール**が層をまたいでも極端に変わらなくなり、深いネットワークの学習が安定します。

---

## オプティマイザと学習率

**オプティマイザ**は、**勾配を使ってパラメータをどう更新するか**を決めるアルゴリズムです。

### 代表的なオプティマイザ

| 名前 | 概要・特徴 |
|------|------------|
| **SGD（確率的勾配降下法）** | 勾配 \( \nabla L \) の方向に、学習率 \( \eta \) で更新：\( \theta \leftarrow \theta - \eta \nabla L \)。シンプルだが、学習率の設定が重要。**モメンタム**を加える（前回の更新の向きを少し残す）と、振動を抑えつつ収束が速くなることが多い。 |
| **Adam** | **モメンタム**と**適応的な学習率**を組み合わせたオプティマイザ。勾配の1次モーメント（平均）と2次モーメント（分散の推定）を保持し、パラメータごとに学習率をスケールする。初期学習率の選択が比較的容易で、多くのタスクで**デフォルト**として使われる。 |
| **AdamW** | Adam に **Weight Decay** を正則化として明示的に組み込んだもの。重みの減衰の扱いが Adam より自然で、**汎化性能**が良くなることがある。 |

**学習率**は、**大きすぎると発散や振動**、**小さすぎると収束が遅い**ため、**学習率スケジューリング**（訓練の進行に合わせて学習率を減らす：Step decay、Cosine annealing、Warmup など）がよく使われます。**Warmup** では、最初の数ステップで学習率を0から徐々に上げ、初期の不安定さを抑えます。

---

## フレームワーク（PyTorch、TensorFlow）

**フレームワーク**は、**ニューラルネットワークの定義・学習・推論**を、計算グラフと**自動微分**によって効率的に実装するためのライブラリです。

### PyTorch

**PyTorch** は、**Define-by-Run**（計算を実行しながら計算グラフが決まる）のスタイルで、**柔軟なデバッグ**と**直感的な記述**がしやすいです。**テンソル計算**と**自動微分（autograd）** が中心にあり、**nn.Module** で層やモデルをクラスとして定義し、**オプティマイザ**と**損失関数**で学習ループを書く流れが一般的です。研究やプロトタイピングで広く使われ、多くの論文の実装が PyTorch で公開されています。

### TensorFlow / Keras

**TensorFlow** は、**静的計算グラフ**（従来）と **Eager Execution**（動的）の両方をサポートし、**本番デプロイ**（TensorFlow Serving、TFLite、TF.js など）や**大規模学習**との連携が強いです。**Keras** は TensorFlow に統合された高レベルAPIで、**モデルを層の積み重ねで簡潔に定義**し、**compile** と **fit** で学習まで一気に書けます。**データパイプライン（tf.data）** や **分散学習** も整っており、プロダクションやエッジデバイスでの利用がしやすいです。

### 開発の流れ（共通）

どちらのフレームワークでも、おおまかには次の流れになります。

1. **データの準備**: データローダーや前処理（正規化、データ拡張など）を用意する。
2. **モデルの定義**: 層・ブロックを組み合わせてネットワークを定義する。
3. **損失・オプティマイザの選択**: タスクに合わせて損失関数とオプティマイザを選ぶ。
4. **学習ループ**: ミニバッチごとに順伝播 → 損失計算 → 逆伝播 → パラメータ更新を繰り返す（Keras なら **fit** で一括）。
5. **評価・推論**: 検証データやテストデータで評価し、必要に応じてモデルを保存・デプロイする。

---

## まとめ

本記事の要点をまとめます。

- **CNN** は、畳み込みとプーリングにより**局所パターン**と**位置のずれへの頑健性**を扱い、**画像・時系列**で広く使われます。ResNet の**残差接続**により、深いCNNが安定して学習できるようになりました。
- **RNN / LSTM** は、**時系列・テキスト**の順序を扱うためのモデルで、隠れ状態で「文脈」を保持します。LSTM は長期依存を学習しやすくしましたが、並列化が難しく、自然言語では **Transformer** に置き換わっています。
- **Transformer** は **Attention** を中心とし、**並列計算**と**長距離依存**の両立が可能です。Query/Key/Value による柔軟な参照と、**Multi-Head Attention**・**Layer Normalization**・**残差接続**が標準となり、NLP をはじめ画像・音声・マルチモーダルでも基盤アーキテクチャです。
- **正則化**（Weight Decay、Dropout）で過学習を抑え、**正規化**（Batch Normalization、Layer Normalization）で中間層の分布を整え、学習を安定させます。
- **オプティマイザ**（SGD、Adam、AdamW）と**学習率スケジューリング**（Warmup、Cosine decay など）により、効率的かつ安定した学習が可能になります。
- **PyTorch** と **TensorFlow/Keras** は、自動微分と計算グラフによりモデルの定義・学習・推論を効率化し、研究から本番デプロイまでを支える基盤となっています。

次の記事では、**ディープラーニングの応用例**として、画像認識・物体検出・セグメンテーション、自然言語処理、音声処理、推薦・生成モデルなど、分野ごとのタスクと代表的なモデルを詳しく解説します。
