---
title: "教師なし学習：クラスタリングと次元削減"
coverImage: "/assets/blog/preview/cover.jpg"
date: "2026-02-01T12:00:00.000Z"
ogImage:
  url: "/assets/blog/preview/cover.jpg"
---

教師なし学習（Unsupervised Learning）は、**正解ラベルなしのデータ**から、データの構造やパターン、グループ分けを自動的に発見する機械学習の手法です。ラベルを付けるコストが高い場合や、まずデータの傾向を把握したい場合に有効です。この記事では、教師なし学習の基本に加え、**階層なしクラスタリング・階層ありクラスタリング・主成分分析・協調フィルタリング・トピックモデル**の5つを、定式化・アルゴリズム・メリット・デメリットまで詳しく解説します。

### この記事でわかること

- 教師なし学習の目的（構造の発見・要約）と、教師あり学習との違い
- 階層なしクラスタリング（K-means、DBSCAN、GMM）と階層ありクラスタリング（凝集型・リンケージ）の考え方と使い分け
- 主成分分析（PCA）の定式化と、標準化・説明率の意味
- 協調フィルタリング（メモリベース・行列分解）とトピックモデル（LDA）の概要と応用

---

## 教師なし学習とは

教師あり学習では「入力と正解のペア」を使って学習しますが、教師なし学習では**正解ラベルが付いていないデータ**だけを入力とします。正解は与えませんが、データそのものの**分布**や**構造**（似たデータのまとまり、ばらつきの方向、異常なパターンなど）を、アルゴリズムが自動的に発見します。その代表的なタスクが、グループ分け（**クラスタリング**）、次元削減、密度推定、異常検知です。

### 教師あり学習との違い

| 項目 | 教師あり学習 | 教師なし学習 |
|------|--------------|--------------|
| 入力 | 特徴量 + 正解ラベル | 特徴量のみ |
| 目的 | 予測・分類 | 構造の発見・要約 |
| 例 | スパム判定、売上予測 | 顧客セグメント、異常検知 |

### 主なタスク

- **クラスタリング（Clustering）**: データを似たもの同士のグループに分ける。
- **次元削減（Dimensionality Reduction）**: 特徴量の数を減らしつつ、情報をできるだけ保つ。
- **密度推定**: データの分布を推定する。
- **異常検知**: 通常と異なるパターン（外れ値）を検出する。

## クラスタリングの概要

クラスタリングは、**ラベルなしでデータをグループ（クラスタ）に分ける**タスクです。**クラスタ**とは、似た特徴を持つデータのまとまりのことで、人間がラベルを付ける代わりに、アルゴリズムが「近いデータ同士を同じグループにまとめる」ことでクラスタを形成します。顧客セグメンテーションや可視化・前処理などに使われます。手法は**階層なし**（平坦なクラスタのみを出力し、K-means や DBSCAN など）と**階層あり**（樹形図で「どのクラスタ同士が近いか」の階層を一度に表現し、後からクラスタ数を決められる）の2系統に分かれます。データの形（球状・任意形状・ノイズの有無）や、クラスタ数を事前に決めたいかどうかで選ぶとよいです。以下、階層なしから順に解説します。

---

## 階層なしクラスタリング（Non-hierarchical Clustering）

階層なしクラスタリングでは、データを**あらかじめ決めた数またはデータ駆動で決まる数のクラスタ**に分割し、階層構造は持ちません。代表的な手法は K-means、DBSCAN、混合ガウスモデル（GMM）です。

### K-means法（K-means）

データを **K個のクラスタ**に分け、各クラスタの中心（**セントロイド**）と各データの所属を交互に更新して収束させます。**セントロイド**とは、そのクラスタに属するデータの重心（平均ベクトル）のことで、K-means では「各データを最も近いセントロイドに割り当てる」と「各クラスタのセントロイドを、そのクラスタ内のデータの平均で更新する」を繰り返します。

#### 定式化

目的は**クラスタ内平方和（Within-Cluster Sum of Squares, WCSS）**、すなわち「各データと、そのデータが属するクラスタのセントロイドとの距離の二乗和」を最小にすることです。各データを最も近いセントロイドに割り当て、各クラスタ内のデータの平均を新たなセントロイドとする更新を、割り当てが変わらなくなるまで繰り返します。

#### アルゴリズムの手順

1. **初期化**: K個のセントロイド \( \mu_1, \ldots, \mu_K \) を、ランダムにK個のデータ点に置く、または k-means++ などで分散させる。
2. **割り当て**: 各データ \( x_i \) を、距離が最小のセントロイド \( \mu_k \) に割り当てる（ユークリッド距離が一般的）。
3. **更新**: 各クラスタ \( C_k \) について、\( \mu_k = \frac{1}{|C_k|} \sum_{x \in C_k} x \) でセントロイドを更新する。
4. **収束**: 割り当てが変わらなくなるまで 2〜3 を繰り返す。

#### クラスタ数Kの決め方

- **エルボー法**: Kを変えたときのWCSS（または慣性）をプロットし、曲線の「ひじ」が曲がるKを選ぶ。
- **シルエット係数**: 各Kでシルエット係数を計算し、最大になるKを候補にする。
- **ドメイン知識**: ビジネス上意味のあるクラスタ数や解釈可能性を考慮する。

#### メリット・デメリット

- **メリット**: シンプルで高速、大規模データにも適用可能、実装が容易。
- **デメリット**: クラスタ数Kを事前に決める必要がある、**球状（あるいは等分散に近い）クラスタ**を仮定するため、細長い形や密度が異なるクラスタには向かない。外れ値に敏感（セントロイドが引きずられる）。初期値によって局所解に陥るため、複数初期値で試すことが推奨される。

---

### DBSCAN（Density-Based Spatial Clustering of Applications with Noise）

**密度**に基づいてクラスタを形成します。密な領域をクラスタとし、疎な領域を**ノイズ（外れ値）** として扱います。クラスタ数Kを指定しません。

#### 基本概念

- **コア点**: 半径 \( \varepsilon \) 以内に**MinPts個以上**の点（自分含む）がある点。
- **直接密度到達**: コア点の \( \varepsilon \) 近傍内の点は、そのコア点から「直接密度到達」。
- **密度到達・密度連結**: 直接密度到達の連鎖でつながる点同士を同一クラスタとする。
- **ノイズ**: どのコア点からも密度到達できない点。クラスタに属さない。

#### ハイパーパラメータ

- **\( \varepsilon \)（eps）**: 近傍の半径。小さいと細かく分かれ、大きいとクラスタがまとまりやすくノイズが減る。
- **MinPts**: コア点とみなすための最小点数。次元が高い場合は大きめにすることが多い（例：2×次元以上）。

#### メリット・デメリット

- **メリット**: クラスタ数を事前に決めなくてよい、**任意の形**のクラスタに対応、外れ値を自動的にノイズとして検出できる。
- **デメリット**: 密度の差が大きいデータでは、密なクラスタと疎なクラスタを同じ \( \varepsilon \) で扱いにくい。\( \varepsilon \) と MinPts の設定に敏感。高次元では「距離」が効きにくくなる次元の呪いの影響を受けやすい。

---

### 混合ガウスモデル（GMM: Gaussian Mixture Model）

データが**複数のガウス分布の混合**から生成されると仮定し、各データがどの分布（クラスタ）に属するかという**所属確率**を推定します。**ソフトクラスタリング**が得られます。

#### モデル

\( K \) 個のガウス分布 \( \mathcal{N}(\mu_k, \Sigma_k) \) と混合比率 \( \pi_k \)（\( \sum_k \pi_k = 1 \)）を考え、データ \( x \) の密度を \( p(x) = \sum_{k=1}^K \pi_k \, \mathcal{N}(x \mid \mu_k, \Sigma_k) \) とします。各 \( x \) について、事後確率 \( P(\text{クラスタ} k \mid x) \) が所属確率として得られます。

#### 推定（EMアルゴリズム）

**Expectation-Maximization（EM）** で、潜在変数（どのクラスタに属するか）を推論しつつ、パラメータ \( \pi_k, \mu_k, \Sigma_k \) を更新します。

- **Eステップ**: 現在のパラメータで、各データのクラスタ所属確率（負担率）を計算する。
- **Mステップ**: 負担率を重みとして、各クラスタの \( \pi_k, \mu_k, \Sigma_k \) を更新する。

収束するまで E と M を繰り返します。

#### メリット・デメリット

- **メリット**: ソフトクラスタリング（所属確率）が得られる、**楕円形**のクラスタ（共分散 \( \Sigma_k \) で形を表現）に対応、K-meansより柔軟な形状。
- **デメリット**: クラスタ数Kなどのパラメータが必要、EMは局所解に陥るため初期値や複数試行が重要、計算コストは K-means より高い。

---

### 階層なしクラスタリングの評価

正解ラベルがないため、教師あり学習のような「正解率」は使えません。代わりに、**クラスタのまとまり具合**や**クラスタ数ごとの指標の変化**を見て評価します。

- **シルエット係数**: 同じクラスタ内では近く、他クラスタとは遠いほど高くなる。−1〜1の範囲。
- **エルボー法（K-means）**: WCSSのKに対する変化で「ひじ」を探す。
- **ドメイン知識**: 得られたクラスタがビジネス上解釈できるかでクラスタ数や手法を検討する。

---

## 階層ありクラスタリング（Hierarchical Clustering）

階層ありクラスタリングでは、データを**樹形図（デンドログラム）** で表し、**どのクラスタ同士が近いか**の階層構造を一度に得ます。切る高さを変えることで、クラスタ数を後から決められます。

### 凝集型（Agglomerative）と分割型（Divisive）

- **凝集型（ボトムアップ）**: 最初は各データを1つのクラスタ（\( n \) 個）とし、**最も近い2つのクラスタ**を順に結合して、最終的に1つのクラスタになるまで繰り返す。実務でよく使われる。
- **分割型（トップダウン）**: 最初は全データを1つのクラスタとし、何らかの基準で分割を繰り返す。計算コストが高くなりがち。

以下は凝集型を前提に説明します。

### クラスタ間距離（リンケージ）

2つのクラスタを「結合するかどうか」決めるために、**クラスタ間の距離**を定義する必要があります。この定義が**リンケージ（linkage）** です。

- **単連結（Single Linkage）**: 2つのクラスタのうち、**最も近い**1対の点の距離。鎖状につながりやすく、**チェイニング**が起きやすい。
- **完全連結（Complete Linkage）**: 2つのクラスタのうち、**最も遠い**1対の点の距離。コンパクトなクラスタになりやすく、外れ値の影響を受けやすい。
- **群平均（Average Linkage）**: 異なるクラスタに属する**すべての点対の距離の平均**。単連結と完全連結の中間的な性質。
- **ウォード法（Ward）**: 結合したときに**クラスタ内平方和の増加が最小**になる2つを結合する。K-meansに近い「まとまり」を重視し、多くの場合バランスが良い。

### デンドログラム（樹形図）

縦軸にクラスタ間距離（結合時の距離）、横軸にデータ点またはクラスタを並べ、結合の様子を樹形図で表したものが**デンドログラム**です。ある高さで横線を引くと、その時点で存在するクラスタ数が決まります。高さの「飛び」が大きいところで切ると、自然なクラスタ数として解釈することが多いです。

### 距離・類似度

データ間の距離には**ユークリッド距離**がよく使われます。特徴量のスケールが違う場合は標準化が必要です。類似度（大きいほど似ている）を使う場合は、距離に変換する（例：距離 = 1 − 類似度）か、リンケージの定義を類似度用に変更する必要があります。

### メリット・デメリット

- **メリット**: クラスタ数を事前に決めず、デンドログラムを見て**後から決められる**。樹形図でデータの階層構造を可視化できる。任意の形状にある程度対応可能（リンケージによる）。
- **デメリット**: 計算量が \( O(n^2) \) や \( O(n^3) \) になり、**大規模データでは重い**。一度結合すると取り消せないため、局所的な最適性しか保証されない。

### クラスタリング手法の選び方（簡易）

| 状況 | 候補 |
|------|------|
| クラスタ数が決まっていて、球状・高速が欲しい | K-means |
| クラスタ数は決めたくない、任意の形・ノイズ検出 | DBSCAN |
| 所属確率（ソフト）が欲しい、楕円形クラスタ | GMM |
| クラスタ数を後から決めたい、階層を見たい | 階層的（凝集型） |

---

## 主成分分析（PCA: Principal Component Analysis）

主成分分析（PCA）は、**特徴量の線形結合で作った互いに無相関な新たな変数（主成分）** を、**分散が大きい順**に取り、**次元を減らしつつ情報（分散）をできるだけ保つ**手法です。なぜ分散が大きい方向を取るかというと、データのばらつきが大きい方向ほど「データの違いを表す情報」が多く含まれており、その方向を残すことで、少ない次元でもデータの構造を保ちやすいからです。可視化、ノイズ削減、前処理などに広く使われます。

### 定式化

データ行列 \( X \)（行がサンプル、列が特徴量）を考え、各列を中心化（平均0）にしたうえで、**第1主成分**はデータの分散を最大にする方向の単位ベクトル \( w_1 \) です。つまり \( \text{Var}(X w_1) \) を最大化する \( \|w_1\| = 1 \) を求めます。**第2主成分**は \( w_1 \) と無相関という条件の下で分散を最大にする方向、同様に第3、第4…と求めてゆきます。

### 固有値分解との関係

中心化したデータの**共分散行列**（または相関行列）を \( \Sigma \) とすると、主成分方向は \( \Sigma \) の**固有ベクトル**、各主成分の分散は対応する**固有値**に等しくなります。固有値の大きい順に固有ベクトルを並べたものが第1、第2、…主成分です。

### 変換と再構成

データ \( x \) を主成分に射影すると \( z = W^\top x \)（\( W \) は主成分ベクトルを列に持つ行列）です。上位 \( k \) 個の主成分だけ使うと、\( k \) 次元に削減された表現が得られます。逆に \( \hat{x} = W_k z_k \) で元の空間に戻すと**再構成**でき、再構成誤差は捨てた主成分の分散の和になります。

### 説明率・累積説明率

各主成分の**説明率**は「その主成分の分散 / 全分散」です。**累積説明率**は上位から順に足したもので、「何次元まで使えば情報の何%を保っているか」の目安になります。例えば累積説明率が95%になる主成分まで使う、といった決め方がよく行われます。

### 標準化の重要性

特徴量の**スケールが大きく違う**場合、分散が大きい変数が主成分を支配しがちです。そのため、**標準化**（平均0・分散1）してからPCAを行うことが推奨されます。相関行列に対するPCAは、標準化したデータの共分散行列に対するPCAと一致します。

### メリット・デメリット

- **メリット**: 数学的に明確で再現性が高い、実装が容易、計算が比較的軽い、説明率で削減次元を議論できる。
- **デメリット**: **線形**な関係しか捉えられない、主成分は元の変数の線形結合なので解釈は「方向」で行う必要がある。外れ値の影響を受ける場合がある（ロバストPCAなどの拡張あり）。

---

## 協調フィルタリング（Collaborative Filtering）

協調フィルタリングは、**ユーザーとアイテム（商品・映画・記事など）の相互作用データ**（評価・クリック・購入など）から、「似たユーザー」や「似たアイテム」の行動を利用して、**未観測の評価や嗜好を予測する**手法です。「協調」とは、多数のユーザーの行動を**協調的に**利用するという意味で、あるユーザーがまだ見ていないアイテムについて、似た趣味の他のユーザーがどう評価したかを手がかりに予測します。推薦システムの代表的なアプローチです。

### ユーザー・アイテム行列

\( R \) を**ユーザー×アイテム**の行列とします。\( R_{ui} \) はユーザー \( u \) がアイテム \( i \) に付けた評価（またはクリック有無など）です。多くの要素は**欠損**（未評価・未閲覧）であり、その欠損部分を予測することが目的になります。

### メモリベース（近傍ベース）

**ユーザー間**または**アイテム間**の類似度を計算し、近いユーザー（またはアイテム）の評価を重み付きで使って予測します。

- **ユーザー協調フィルタリング**: ユーザー \( u \) と似たユーザーたちの、アイテム \( i \) に対する評価の加重平均で \( R_{ui} \) を予測する。「似た趣味の人が高く評価しているなら、このユーザーも高く評価しそう」という考え方。
- **アイテム協調フィルタリング**: アイテム \( i \) と似たアイテムたちについて、ユーザー \( u \) の評価の加重平均で \( R_{ui} \) を予測する。「似たアイテムを高く評価しているなら、このアイテムも高く評価しそう」という考え方。

類似度には**ピアソン相関**や**コサイン類似度**がよく使われます。欠損を除いて計算する必要があります。

### モデルベース：行列分解（Matrix Factorization）

**行列 \( R \) を低ランクに近似**する方法です。ユーザーを \( d \) 次元の潜在ベクトル \( p_u \)、アイテムを \( d \) 次元の潜在ベクトル \( q_i \) で表し、\( R_{ui} \approx p_u^\top q_i \) とモデル化します。観測された \( (u, i) \) についての二乗誤差（など）を最小化して \( p_u, q_i \) を学習し、未観測の \( R_{ui} \) を \( p_u^\top q_i \) で予測します。**SVD**や**NMF**、**確率的行列分解（PMF）** などがこの枠組みに含まれます。

### 冷たいスタート（Cold Start）

- **新規ユーザー**: 評価履歴がほとんどないため、類似ユーザーが決めにくく、協調フィルタリングだけでは予測が難しい。
- **新規アイテム**: 誰にも評価されていないと、類似アイテムが決めにくい。

対策として、**コンテンツ情報**（ユーザー属性・アイテムの特徴）を組み合わせた**ハイブリッド推薦**や、初期表示では人気アイテム・ランダム推薦を使うなどの方法があります。

### メリット・デメリット

- **メリット**: 正解ラベル（教師信号）が「ユーザーの行動」だけで得られる、ドメインに依存しにくい。
- **デメリット**: 冷たいスタートに弱い、スパースな行列では類似度や分解の推定が不安定になりやすい、解釈は「似たユーザー/アイテム」程度に限られる。

---

## トピックモデル（Topic Model）

トピックモデルは、**文書集合**を「**トピック**（話題）の混合」として表現する教師なしのモデルです。ここで**トピック**とは、特定の単語が出現しやすい確率分布で表される「話題」のことで、人間がラベルを付けるのではなく、データから「この文書群にはこうした話題のまとまりがある」を自動的に発見します。各文書がどのトピックをどれだけ含むか、各トピックがどの単語をどれだけ出すかを推定し、文書の要約・分類・検索などに使われます。

### Bag of Words（BoW）

文書を**単語の出現回数のベクトル**（または出現有無）で表す表現です。語順は捨て、単語の頻度だけを考えます。トピックモデルでは多くの場合、このBoWを入力とします。

### 潜在的ディリクレ配分（LDA: Latent Dirichlet Allocation）

LDAは代表的なトピックモデルです。

- **文書－トピック**: 各文書 \( d \) は、\( K \) 個のトピックに対する**混合比率** \( \theta_d \)（ディリクレ分布から生成）を持つ。文書 \( d \) の各単語は、まず \( \theta_d \) に従ってトピック \( z \) が選ばれ、そのトピックに対応する単語分布から単語が生成されると仮定する。
- **トピック－単語**: 各トピック \( k \) は、語彙上の**単語分布** \( \phi_k \)（ディリクレ分布から生成）を持つ。トピック \( k \) が選ばれたら、\( \phi_k \) に従って単語が1つ選ばれる。

ハイパーパラメータとして、**トピック数 \( K \)**、文書－トピックのディリクレのパラメータ \( \alpha \)、トピック－単語のディリクレのパラメータ \( \beta \)（または \( \eta \)）を設定します。

### 推定（ギブスサンプリング・変分推論）

真の事後分布は計算が難しいため、**ギブスサンプリング（MCMC）** や**変分ベイズ**などで、文書－トピックの分布 \( \theta_d \) とトピック－単語の分布 \( \phi_k \) を近似推定します。推定結果から、各文書のトピック比率（トピックの混合）と、各トピックを代表する単語リストが得られます。

### 解釈と利用

- **文書の要約**: 文書のトピック比率を見て、「この文書はトピック1が60%、トピック2が30%…」のように要約できる。
- **類似文書**: トピック比率ベクトル同士の類似度（コサイン類似度など）で文書の類似度が計算できる。
- **トピックのラベリング**: 各トピックで確率の高い単語を見て、人間が「政治」「スポーツ」などのラベルを付けて解釈する。

### メリット・デメリット

- **メリット**: 教師なしで文書の構造（トピック）を発見できる、解釈可能なトピックと単語リストが得られる、推薦・検索・分類の前処理や特徴量として使える。
- **デメリット**: トピック数 \( K \) を事前に決める必要がある、BoWなので語順・文脈は使わない、推定は計算コストがかかる、短い文書では推定が不安定になりやすい。

---

## その他の手法（次元削減・異常検知）

PCAは線形な次元削減でしたが、**非線形**な構造を可視化したい場合は次のような手法が使われます。

### t-SNE・UMAP

**t-SNE**は近傍関係を保ちながら2次元（または3次元）に写像し、可視化によく使われます。**UMAP**は多様体を仮定した次元削減で、t-SNEより高速で大規模データにも適用しやすい場合があります。どちらも非線形な構造の可視化に適しています。

## 異常検知（Anomaly Detection）

教師なし学習では、**通常のパターンから外れたデータ（外れ値）** を検出する異常検知にも応用されます。

### 主なアプローチ

- **統計的手法**: 正規分布を仮定し、一定の閾値（例：3σ）を超えたものを異常とする。
- **距離ベース**: 近傍が少ない、または距離が大きい点を異常とする（例：k-NN、LOF）。
- **密度ベース**: 密度が極端に低い領域の点を異常とする。
- **再構成ベース**: オートエンコーダなどでデータを再構成し、再構成誤差が大きいものを異常とする。

### 応用例

- 不正検知（クレジットカード、ログイン）
- 故障・異常の検知（センサーデータ、製造）
- ネットワーク侵入検知

## 教師なし学習の実践のポイント

教師なし学習では正解がないため、「何を目的にするか」を事前に決めておくことが重要です。

1. **目的の明確化**: クラスタリングで「何をしたいか」（セグメンテーション、可視化、前処理など）を決める。
2. **特徴量の設計**: スケールの違いを正規化する、ドメインに合った特徴量を用意する。
3. **アルゴリズムとパラメータ**: データの性質（クラスタの形、サイズ、ノイズ）に合わせてアルゴリズムとパラメータを選ぶ。
4. **解釈と検証**: 得られたクラスタや次元削減結果がビジネスや現象の解釈と合っているか確認する。

## まとめ

本記事の要点をまとめます。

- 教師なし学習は、**正解ラベルなしのデータ**から、データの構造やパターン（グループ分け、ばらつきの方向、異常など）を自動的に発見する枠組みです。
- **階層なしクラスタリング**（K-means、DBSCAN、GMM）では、平坦なクラスタに分割します。K-meansは球状・高速、DBSCANはクラスタ数不要・任意形状・ノイズ検出、GMMはソフトクラスタリング（所属確率）が得られます。
- **階層ありクラスタリング**（凝集型・分割型）では、デンドログラム（樹形図）で階層を可視化し、リンケージ（単連結・完全・群平均・ウォード）でクラスタ間距離を定義します。クラスタ数を後から決められる点が利点です。
- **主成分分析（PCA）** は、分散が大きい方向（主成分）を順に取り、線形に次元削減します。固有値・説明率で削減次元を決め、特徴量のスケールが異なる場合は標準化が重要です。
- **協調フィルタリング**は、ユーザー×アイテムの相互作用から未観測の評価を予測する手法です。メモリベース（ユーザー/アイテム近傍）と行列分解（潜在ベクトル）があり、新規ユーザー・新規アイテム（冷たいスタート）が課題です。
- **トピックモデル（LDA）** は、文書をトピックの混合として表現し、文書－トピック・トピック－単語の分布を推定します。文書の要約・類似文書・トピックの解釈に使えます。
- **異常検知**にも、教師なし学習（統計・距離・密度・再構成ベース）が広く使われます。

次の記事では、**強化学習**のバンディットアルゴリズム、マルコフ決定過程、価値関数、方策勾配、および代表的なアルゴリズムについて解説します。
