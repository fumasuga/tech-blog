---
title: "モデルの選択・評価"
coverImage: "/assets/blog/preview/cover.jpg"
date: "2026-02-02T12:00:00.000Z"
ogImage:
  url: "/assets/blog/preview/cover.jpg"
---

機械学習では、**どのモデルを選ぶか**と**そのモデルがどれだけ良いか**を客観的に判断する必要があります。そのために、**データの扱い方**（訓練・検証・テストの分割、交差検証）と**評価指標**（予測誤差、正解率・適合率・再現率・F値、ROC曲線とAUC）、さらに**モデルの選択と情報量基準**（AIC、BIC、交差検証によるモデル選択）を押さえておくことが重要です。この記事では、これらを詳しく解説します。

### この記事でわかること

- データの分割（訓練・検証・テスト）と交差検証の考え方と使い分け
- 回帰の評価指標：予測誤差（MSE、MAE、RMSE、MAPE、R²）
- 分類の評価指標：正解率、適合率、再現率、F値（F1）、マクロ・マイクロ
- ROC曲線とAUCの意味と解釈
- モデルの選択と情報量基準（AIC、BIC）の考え方と使い方

---

## データの扱い

モデルを学習し、その性能を評価するためには、**どのデータで学習し、どのデータで評価するか**を明確に分ける必要があります。学習に使ったデータでそのまま評価すると、**過学習**したモデルでも「良いスコア」が出てしまい、未知データでの性能を過大評価してしまいます。そのため、**訓練用・検証用・テスト用**にデータを分割し、評価は**学習に使っていないデータ**で行います。

### 訓練・検証・テストの分割

- **訓練データ（Training data）**: モデルの**パラメータを学習する**ために使うデータ。ここでだけモデルが「学習」する。
- **検証データ（Validation data）**: ハイパーパラメータの調整や**モデル選択**（どのモデル・どの設定を選ぶか）のために使うデータ。学習には使わず、複数の候補モデルや設定を比較するときに使う。
- **テストデータ（Test data）**: **最終的な性能評価**のためにだけ使うデータ。学習にも検証にも一切使わず、最後に1回だけ評価に使う。これにより、「未知データでの性能」の不偏な推定ができる。

典型的な割合は、**訓練 6〜7割・検証 1〜2割・テスト 1〜2割**です。データ量が少ない場合は検証とテストを兼用したり、交差検証で検証の役割を担わせたりすることがあります。

#### なぜ検証とテストを分けるか

検証データで「どのモデルが良いか」を何度も見てモデルやハイパーパラメータを選ぶと、**検証データに indirect に fitting している**状態になります。そのため、検証データでのスコアは楽観的になりがちです。**テストデータ**は一度も選択の判断に使わないため、最終的な性能を**公平に**評価するために使います。

### 交差検証（Cross-Validation）

データ量が少ない場合、単純に1回だけ訓練・検証に分割すると、検証の結果が**分割の仕方に依存**して不安定になりがちです。**交差検証**では、データを \( K \) 個のブロック（**フォールド**）に分け、そのうち1つを検証・残りを訓練として \( K \) 通り試し、検証スコアの**平均**を取ります。これにより、分割の偶然性を減らし、**汎化性能**の推定を安定させます。

#### K分割交差検証（K-Fold CV）

1. データを \( K \) 個のフォールドに分ける（例：\( K=5 \) なら5等分）。
2. \( i = 1, \ldots, K \) について、第 \( i \) フォールドを検証、残り \( K-1 \) 個を訓練としてモデルを学習し、検証データでスコアを計算する。
3. \( K \) 個の検証スコアの**平均**（と標準偏差）を、そのモデル・設定の性能の推定とする。

**stratified K-Fold（層化K分割）** では、分類問題で**各フォールド内のクラス比率**が元のデータとほぼ同じになるように分割します。クラスが偏っているデータでは、層化しないと「あるフォールドに陽性がほとんどいない」といった偏りが起き、評価が不安定になるため、層化が推奨されます。

#### 交差検証の注意点

- 交差検証で得たスコアは「検証」の役割であり、**モデル選択やハイパーパラメータ選択**に使います。**最終的な報告用の性能**は、別に確保したテストデータで評価するのが望ましいです。
- 時系列データでは、**未来のデータで訓練・過去のデータで検証**といった分割はしないため、**時系列順に訓練・検証を分ける**（例：最初の8割を訓練、残り2割を検証）か、時系列を考慮した交差検証（Time Series Split など）を使います。

### データの前処理と評価の関係

**正規化・標準化・欠損値補完・特徴量変換**などは、**訓練データだけ**から統計量（平均・標準偏差など）を計算し、その同じ変換を検証・テストデータに適用します。検証・テストデータから統計量を計算してしまうと、**未来の情報が学習に漏れている**ことになり、評価が楽観的になります。交差検証でも、各フォールドの**訓練部分だけ**から前処理のパラメータを決め、検証部分には訓練部分の統計量で変換を適用します。

---

## 評価指標

**評価指標**は、モデルの予測が正解にどれだけ近いか（回帰）またはどれだけ正しく分類しているか（分類）を数値化したものです。タスクの種類と、**何を重視するか**（全体の正しさか、陽性の取りこぼしを減らすかなど）に応じて指標を選びます。

### 回帰の評価指標：予測誤差

回帰では、予測値 \( \hat{y} \) と実測値 \( y \) の**誤差**を何らかの形で集約した指標を使います。

#### 平均二乗誤差（MSE: Mean Squared Error）

\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

予測値と実測値の差（**残差**）の**二乗の平均**です。大きな誤差ほど強くペナルティがかかり、数学的にも扱いやすいため、回帰の損失関数としてよく使われます。単位は元の \( y \) の**二乗**になるため、解釈時には次の RMSE に変換することが多いです。

#### 平均絶対誤差（MAE: Mean Absolute Error）

\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]

残差の**絶対値の平均**です。外れ値の影響を MSE より受けにくく、「平均してどれだけずれているか」を直感的に解釈しやすいです。単位は \( y \) と同じです。

#### 二乗平均平方根誤差（RMSE: Root Mean Squared Error）

\[
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\]

MSE の平方根です。単位が \( y \) と一致し、「典型的な誤差の大きさ」として解釈しやすいです。MSE と同様、大きな誤差ほど重く効きます。

#### 平均絶対パーセント誤差（MAPE: Mean Absolute Percentage Error）

\[
\text{MAPE} = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right| \quad (y_i \neq 0)
\]

誤差を**実測値の何%か**で表したものの平均です。スケールが異なる変数どうしの誤差を相対的に比較するときに使います。\( y_i \) が0に近いと MAPE が不安定になるため、その場合は使わないか、別の指標と併用します。

#### 決定係数（R²、R-squared）

\[
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\text{残差平方和}}{\text{全平方和}}
\]

\( \bar{y} \) は実測値の平均です。**「実測値のばらつき」のうち、モデルで説明できた割合**を表します。1に近いほど良く、0は「平均を予測するのと同じ程度」、負になることもありその場合は平均より悪いことを意味します。スケールに依存しないため、異なるデータセット間のモデル比較に便利です。

| 指標 | 特徴 | 主な用途 |
|------|------|----------|
| MSE | 二乗和、大きな誤差に敏感、単位は \( y^2 \) | 損失関数、学習 |
| MAE | 絶対値、外れ値に頑健、単位は \( y \) | 解釈、外れ値が多いとき |
| RMSE | MSE の平方根、単位は \( y \) | 典型的な誤差の大きさの解釈 |
| MAPE | 相対誤差の%、スケール違いの比較 | \( y \neq 0 \) で相対比較 |
| R² | 0〜1付近、説明率 | モデル比較、報告 |

---

### 分類の評価指標：正解率・適合率・再現率・F値

分類では、**予測したクラス**と**正解のクラス**の一致度を表す指標を使います。二値分類（陽性・陰性）を前提に説明し、多クラスへの拡張（マクロ・マイクロ）も述べます。

#### 混同行列（Confusion Matrix）

評価指標は多くの場合、**混同行列**から計算されます。二値分類では、次の4つのセルの数を数えます。

|  | 予測：陽性 | 予測：陰性 |
|--|------------|------------|
| **実際：陽性** | **True Positive (TP)** | **False Negative (FN)** |
| **実際：陰性** | **False Positive (FP)** | **True Negative (TN)** |

- **TP**: 実際は陽性で、正しく陽性と予測した数。
- **FN**: 実際は陽性だが、陰性と予測した数（**取りこぼし**）。
- **FP**: 実際は陰性だが、陽性と予測した数（**見逃し**）。
- **TN**: 実際は陰性で、正しく陰性と予測した数。

#### 正解率（Accuracy）

\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} = \frac{\text{正しく予測した数}}{\text{全体}}
\]

**全体のうち、正しく予測した割合**です。直感的ですが、**クラスが偏っているデータ**（例：陽性が1%しかない）では、常に陰性と予測するだけでも正解率は99%になり、指標として誤解を招くことがあります。そのため、陽性・陰性の両方をどう扱っているかを見る**適合率・再現率・F値**を併用することが推奨されます。

#### 適合率（Precision）

\[
\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{陽性と予測したうち、実際に陽性だった数}}{\text{陽性と予測した数}}
\]

**「陽性」と予測したもののうち、実際に陽性だった割合**です。「陽性と言い張ったときの信頼度」と解釈できます。FP が少ないほど適合率は高くなります。陽性をなるべく外さない（誤検知を減らす）場面で重視します。

#### 再現率（Recall）

\[
\text{Recall} = \frac{TP}{TP + FN} = \frac{\text{実際に陽性だったうち、陽性と予測した数}}{\text{実際に陽性だった数}}
\]

**実際に陽性だったもののうち、陽性と予測して捉えられた割合**です。「取りこぼしの少なさ」と解釈できます。FN が少ないほど再現率は高くなります。病気の検出など、**陽性を見逃したくない**場面で重視します。**感度（Sensitivity）** と同じ意味です。

#### F値（F1スコア、F-measure）

\[
F_1 = \frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2 \, TP}{2 \, TP + FP + FN}
\]

**適合率と再現率の調和平均**です。どちらか一方だけが高くても F1 は伸びにくく、**適合率と再現率のバランス**を一つの数値で表したいときに使います。クラス不均衡があるデータでも、陽性クラスに注目した評価がしやすくなります。

より一般に、**F\( \beta \) スコア**は再現率を適合率の \( \beta \) 倍重視した指標です。\( \beta > 1 \) なら再現率をより重視、\( \beta < 1 \) なら適合率をより重視します。\( \beta = 1 \) が F1 です。

#### マクロ・マイクロ平均（多クラス分類）

クラスが3つ以上あるときは、**各クラスを陽性・それ以外を陰性**とみなして適合率・再現率・F1を計算し、それらを**マクロ平均**（各クラスで出した指標の単純平均）または**マイクロ平均**（全サンプルで TP, FP, FN を集計してから適合率・再現率・F1を計算）でまとめます。

- **マクロ平均**: 各クラスを対等に扱う。クラスが少ないクラスでも同じ重みで効く。
- **マイクロ平均**: サンプル数が多いクラスの影響が大きい。全体の正解数に近い感覚。

---

### ROC曲線とAUC

**ROC曲線**（Receiver Operating Characteristic curve）と**AUC**（Area Under the Curve）は、分類モデルが**陽性・陰性をどれだけうまく識別しているか**を、**閾値に依存しない形**で評価するための指標です。モデルが**スコアや確率**を出力する場合（ロジスティック回帰、多くのニューラルネットなど）に使います。

#### 閾値と混同行列

二値分類では、スコアや確率 \( p \) に**閾値**（例：0.5）を設定し、\( p \geq \) 閾値なら陽性、\( p < \) 閾値なら陰性と予測します。閾値を変えると、TP・FP・FN・TN の数が変わり、適合率・再現率も変わります。

#### 真陽性率（TPR）と偽陽性率（FPR）

- **真陽性率（TPR）** = 再現率 = \( TP / (TP + FN) \)。「実際の陽性のうち、陽性と予測した割合」。
- **偽陽性率（FPR）** = \( FP / (FP + TN) \)。「実際の陰性のうち、誤って陽性と予測した割合」。

閾値を 1 から 0 に動かすと、TPR と FPR はともに増えていきます。

#### ROC曲線

**ROC曲線**は、横軸に**FPR**、縦軸に**TPR** を取り、閾値を連続的に変えたときの (FPR, TPR) の軌跡をプロットした曲線です。

- **理想的なモデル**: 陽性サンプルがすべて陰性サンプルより高いスコアを持つとき、閾値を適切に選べば (FPR=0, TPR=1) を達成でき、曲線は左上に張り付く。
- **ランダムな予測**: スコアが正解と無関係なら、曲線は対角線（FPR = TPR）に沿う。
- **良いモデル**: 曲線が**左上**にふくらむほど、同じ FPR でより高い TPR を得られている、つまり「陽性を漏らさず、陰性を陽性と言い間違えない」バランスが良い。

#### AUC（Area Under the ROC Curve）

**AUC**は、ROC曲線の**下側の面積**です。0〜1の値を取り、

- **AUC = 1**: 完全な識別（ある閾値で全サンプルを正しく分類できる）。
- **AUC = 0.5**: ランダムな予測と同程度（対角線の下の面積は0.5）。
- **AUC > 0.5**: ランダムより良い。0.7〜0.8 は「まずまず」、0.8以上は「良い」と解釈されることが多いです。

AUCは**閾値に依存しない**ため、「閾値をまだ決めていない段階」や「閾値を変える運用を考える段階」で、モデルそのものの**識別能力**を比較するのに便利です。また、**クラス不均衡**があるデータでも、陽性が少なくても AUC はある程度安定して解釈できます（正解率だけでは不十分な場合の補完）。

#### PR曲線（Precision-Recall curve）との違い

横軸に**再現率**、縦軸に**適合率**を取った曲線を**PR曲線**といいます。陽性が非常に少ない（不均衡が強い）ときは、ROC曲線は楽観的に見えがちなため、**PR曲線**やその下の面積（**AUPRC**）を併用することが推奨されます。

---

## モデルの選択と情報量基準

**モデルの選択**とは、候補となる複数のモデル（または同じモデルの異なるハイパーパラメータ・特徴量の組み合わせ）のうち、**どれを採用するか**を決めることです。「訓練データへの当てはまり」だけを良くすると、複雑なモデルほど有利になり、**過学習**して未知データでの性能が落ちることがあります。そのため、**当てはまりの良さ**と**モデルの複雑さ**の両方を考慮した指標が使われます。その代表が**情報量基準**（AIC、BIC）と**交差検証**です。

### 過学習と汎化

- **訓練誤差**: 訓練データでの誤差や損失。モデルを複雑にすると、訓練誤差は小さくなりがちです。
- **汎化誤差**: **未知データ**（同じ分布から得られるが、学習には使っていないデータ）での誤差。本当に知りたいのはこちらです。
- **過学習**: 訓練誤差は小さいが、汎化誤差が大きい状態。モデルが訓練データのノイズまで覚えてしまったときに起こります。

モデル選択の目的は、**汎化誤差が小さくなる**モデルを選ぶことです。訓練誤差だけで選ぶと、複雑なモデルが選ばれやすく過学習に陥りやすいため、**複雑さにペナルティをかける**情報量基準や、**学習に使っていないデータ**で評価する交差検証が使われます。

### AIC（赤池情報量基準）

**AIC**（Akaike Information Criterion）は、**当てはまりの良さ**（対数尤度の大きさ）と**パラメータ数**（モデルの複雑さ）のバランスを取る指標です。

\[
\text{AIC} = -2 \times (\text{最大対数尤度}) + 2 \times (\text{パラメータ数}) = -2 \, \ell(\hat{\theta}) + 2k
\]

\( \ell(\hat{\theta}) \) は推定したパラメータ \( \hat{\theta} \) での対数尤度、\( k \) はパラメータの数です。**AICが小さいモデルほど良い**と解釈します。対数尤度が大きい（当てはまりが良い）ほど第一項は小さくなりますが、パラメータ数が増えると第二項が増え、複雑なモデルにペナルティがかかります。統計モデル（回帰、時系列のARモデルなど）の**モデル選択**で広く使われます。

#### なぜ「2×パラメータ数」か

理論的には、AICは**未知データでの対数尤度の期待値**を近似したものと解釈でき、その近似の過程で「2k」のペナルティが出てきます。直感的には、「パラメータが1つ増えると、当てはまりは良くなるが、その分だけ過学習のリスクが増える」ことをペナルティで反映していると考えられます。

### BIC（ベイズ情報量基準）

**BIC**（Bayesian Information Criterion）は、AICと同様に**対数尤度**と**パラメータ数**を使いますが、パラメータ数へのペナルティが**サンプル数 \( n \) に依存**します。

\[
\text{BIC} = -2 \times (\text{最大対数尤度}) + (\text{パラメータ数}) \times \ln n = -2 \, \ell(\hat{\theta}) + k \ln n
\]

**BICが小さいモデルほど良い**と解釈します。\( n \) が大きいときは \( \ln n > 2 \) なので、BICはAICより**パラメータ数に厳しめ**のペナルティをかけ、よりシンプルなモデルを選びがちです。時系列の次数選択（ARの \( p \) など）や、混合数やクラスタ数の選択にも使われます。

### 交差検証によるモデル選択

**交差検証**では、データを訓練・検証に分割し、**検証データでのスコア**（誤差や正解率・F1・AUCなど）でモデルやハイパーパラメータを比較します。学習に使っていないデータで評価するため、**汎化性能**の推定に直結し、情報量基準が使えない（非線形モデル・ブラックボックスモデルなど）場合でも適用できます。

手順は、候補モデル（または設定）ごとに K分割交差検証を行い、**検証スコアの平均**が最も良いモデルを選びます。最終的な性能の報告には、別に確保した**テストデータ**で評価するのが望ましいです。

### 情報量基準と交差検証の使い分け

| 観点 | AIC・BIC | 交差検証 |
|------|----------|----------|
| 計算コスト | 1回の学習で計算できる | K回学習が必要 |
| 解釈 | 尤度とパラメータ数に基づく | 検証データでの直接的なスコア |
| 適用しやすさ | 尤度が定義されたモデル向き | 任意のモデル・任意の指標に使える |
| サンプル数 | BICは \( n \) が大きいとシンプルなモデルを選びがち | データ量が少ないときは分割の影響が大きい |

時系列のAR次数の選択など、**尤度がはっきりしているモデル**では AIC・BIC がよく使われます。機械学習の多くのアルゴリズム（ランダムフォレスト、勾配ブースティングなど）では尤度を直接扱わないため、**交差検証**でモデル選択を行うことが一般的です。

---

## まとめ

本記事の要点をまとめます。

- **データの扱い**: 学習には**訓練データ**だけを使い、性能の評価は**学習に使っていないデータ**（検証・テスト）で行います。**訓練・検証・テスト**の3分割と、**交差検証**（K分割、層化）を状況に応じて使い分けます。前処理の統計量は訓練データからだけ計算し、検証・テストに適用します。
- **回帰の評価指標**: **予測誤差**として MSE、MAE、RMSE、MAPE、**決定係数 R²** を紹介しました。MSE・RMSE は大きな誤差に敏感、MAE は外れ値に頑健、R² は説明率としてモデル比較に便利です。
- **分類の評価指標**: **正解率**は直感的だがクラス不均衡に弱いため、**適合率**（陽性と予測したときの信頼度）、**再現率**（陽性の取りこぼしの少なさ）、**F値**（適合率と再現率の調和平均）を併用します。多クラスではマクロ・マイクロ平均でまとめます。
- **ROC曲線とAUC**: 閾値に依存せず、モデルの**識別能力**を評価するために使います。ROC曲線は (FPR, TPR) の軌跡、**AUC** はその下の面積で、0.5 がランダム、1 が完全な識別です。
- **モデルの選択**: **過学習**を防ぎ**汎化性能**の良いモデルを選ぶために、**AIC**・**BIC**（尤度とパラメータ数に基づく）や**交差検証**（検証データでのスコアに基づく）を使います。尤度が定義された統計モデルでは AIC・BIC、機械学習の多くのモデルでは交差検証が一般的です。

教師あり学習のアルゴリズムを学んだあとで、**どのデータで評価するか**と**どの指標で比較するか**を押さえておくと、実務でモデルを選び・報告する際に役立ちます。

**教師あり**はラベル付きデータで予測、**教師なし**はラベルなしで構造発見、**強化学習**は報酬で方策を学習する、という違いを押さえたうえで、評価指標とモデル選択まで身につけて、興味のある分野から実際のデータやシミュレーターで手を動かしながら学んでみることをおすすめします。
