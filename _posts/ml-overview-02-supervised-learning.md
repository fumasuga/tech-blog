---
title: "教師あり学習：分類と回帰"
coverImage: "/assets/blog/preview/cover.jpg"
date: "2026-02-01T11:00:00.000Z"
ogImage:
  url: "/assets/blog/preview/cover.jpg"
---

教師あり学習（Supervised Learning）は、**正解ラベル付きのデータ**から学習し、新しいデータに対する予測を行う機械学習の手法です。分類（クラス予測）と回帰（数値予測）の2大タスクがあり、実務で最もよく使われる枠組みの一つです。この記事では、教師あり学習の基本に加え、**線形回帰・ロジスティック回帰・ランダムフォレスト・ブースティング・サポートベクターマシン・自己回帰モデル**の6つを、式・仮定・メリット・デメリットまで詳しく解説します。

### この記事でわかること

- 教師あり学習の流れ（訓練データ→学習→予測）と、分類・回帰の違い
- 6つのアルゴリズムの定式化、推定方法、および使い分けの目安
- 分類・回帰の評価指標と、過学習への対策

---

## 教師あり学習とは

教師あり学習では、**入力（特徴量）と正解（ラベル）のペア**を多数与え、入力から正解を予測する関数（**モデル**）を学習します。ここで**特徴量**とはモデルに入力する変数（説明変数）のことで、**ラベル**とは正解の値やクラスです。「教師」とはこの正解のことを指し、正解が付いたデータで学習するため「教師あり」と呼ばれます。

### 学習の流れ

1. **訓練データの準備**: 入力 \( x \) と正解 \( y \) のペア \( (x_1, y_1), (x_2, y_2), \ldots \) を用意する。
2. **モデルの学習**: モデルが \( x \) から \( y \) をできるだけ正確に予測するように、パラメータを調整する。
3. **予測**: 新しい入力 \( x_{\text{new}} \) に対して、学習したモデルで \( y_{\text{new}} \) を予測する。

### 2つの主なタスク：分類と回帰

| タスク | 出力の種類 | 例 |
|--------|------------|-----|
| **分類（Classification）** | 離散的なクラス（カテゴリ） | スパムか否か、画像が猫か犬か |
| **回帰（Regression）** | 連続的な数値 | 株価、気温、売上予測 |

## 分類と回帰の概要

**分類**は、入力がどのクラスに属するかを予測するタスク（二値分類・多クラス分類）です。**回帰**は、入力から連続的な数値を予測するタスクです。どちらも「入力 \( x \) から出力 \( y \) を予測する」という形ですが、\( y \) が離散か連続かで扱うアルゴリズムや評価指標が変わります。以下では、教師あり学習でよく使われる**6つのアルゴリズム**を、回帰系（線形回帰・自己回帰）と分類系（ロジスティック回帰・ランダムフォレスト・ブースティング・SVM）に分けて詳しく解説します。

## 代表的なアルゴリズムの詳細

各アルゴリズムは「モデル式・推定方法・仮定・メリット・デメリット」の順で説明します。**解釈のしやすさ**を重視するか**予測精度**を重視するか、**データの規模や形**（線形・非線形、時系列かどうか）に応じて選ぶとよいです。

### 線形回帰（Linear Regression）

線形回帰は、**目的変数 \( y \) を説明変数 \( x \) の線形結合で予測する**最も基本的な回帰モデルです。

#### モデル式

1つの説明変数の場合（単回帰）は \( y = \beta_0 + \beta_1 x + \varepsilon \) 、複数の説明変数の場合（重回帰）は \( y = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p + \varepsilon \) と表します。\( \beta_0 \) は切片、\( \beta_1, \ldots, \beta_p \) は偏回帰係数、\( \varepsilon \) は誤差項です。

#### 最小二乗法（OLS）

係数 \( \beta \) の推定には**最小二乗法（Ordinary Least Squares, OLS）**が使われます。予測値と実測値の差を**残差**といい、その**二乗和**を最小にする \( \beta \) を求めます。二乗和にすることで、大きな誤差ほど強くペナルティがかかり、また数学的に扱いやすくなります（微分可能で、解が一意に求まる条件が整いやすい）。正規方程式を解くことで、反復計算なしに閉じた形で解が得られます。

#### 仮定と解釈

- **線形性**: \( y \) と \( x \) の関係が線形であること。
- **誤差の独立性・等分散性**: 誤差が互いに独立で、分散が一定であること（外れ値や不均一分散があると推定が不安定になることがある）。
- **多重共線性**: 説明変数同士の相関が非常に高いと、係数の推定が不安定になる。その場合はリッジ回帰や変数選択が有効。

係数 \( \beta_j \) は「\( x_j \) が1単位増えたとき、他変数が一定なら \( y \) が \( \beta_j \) だけ増える」と解釈できます。説明変数を標準化すると、係数の大きさで「どの説明変数が効いているか」を比較しやすくなります。

#### 正則化：リッジ回帰・Lasso回帰

係数が大きくなりすぎると、訓練データの細かいノイズまで fitting し、**過学習**（未知データでの性能低下）が起きやすくなります。そこで、損失に**係数に対するペナルティ**を加え、係数を抑える手法を**正則化**といいます。

- **リッジ回帰**: 損失に \( \lambda \sum_j \beta_j^2 \)（L2正則化）を加え、係数を全体的に小さくする。説明変数同士の相関が高い**多重共線性**のときも推定が安定しやすく、過学習を抑える。
- **Lasso回帰**: \( \lambda \sum_j |\beta_j| \)（L1正則化）を加え、係数を0にしやすくする。その結果、不要な特徴量の係数が0になり、**特徴量選択**が同時に行われる。
- **Elastic Net**: L1とL2を組み合わせた正則化。相関の高い説明変数が多数ある場合などに有用。

---

### ロジスティック回帰（Logistic Regression）

名前には「回帰」とありますが、**二値分類**（多クラスにも拡張可能）に使われる線形モデルです。クラスに属する**確率**を出力するため、閾値を変えることで「陽性と判定する基準」を調整でき、実務で重宝されます。

#### モデル式とシグモイド関数

線形結合 \( z = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p \) を**シグモイド関数** \( \sigma(z) = 1 / (1 + e^{-z}) \) に通し、\( P(y=1|x) = \sigma(z) \) とします。シグモイド関数は \( z \) を 0〜1 の範囲に押し込める関数で、その値を**クラス1に属する確率**として解釈します。\( z \) が大きいほど確率は1に近づき、小さいほど0に近づきます。出力が0〜1の確率なので、閾値（例：0.5）を決めれば、その閾値以上を陽性・未満を陰性とするクラス判定ができます。

#### 最尤推定と交差エントロピー損失

係数 \( \beta \) の推定には**最尤推定**が使われます。二値分類では **交差エントロピー損失（ロジスティック損失）** を最小化する問題と等価です。勾配降下法やニュートン法などで反復的に \( \beta \) を更新します。

#### 多クラスへの拡張

クラスが3つ以上のときは **多項ロジスティック回帰（Softmax回帰）** を使います。各クラスに対して線形のスコアを計算し、Softmax関数で確率に変換し、最大確率のクラスを予測します。

#### メリット・デメリット

- **メリット**: 係数の符号や大きさから「どの特徴が正/負に効くか」が解釈しやすい。計算が軽く、過学習しにくい。正則化（L1/L2）を入れやすい。
- **デメリット**: 決定境界が線形のため、非線形な関係は表現しにくい。特徴量の交互作用を使う場合は、事前に交互作用項を追加する必要がある。

---

### ランダムフォレスト（Random Forest）

ランダムフォレストは、**複数の決定木を組み合わせたアンサンブル手法**で、分類・回帰の両方に使えます。

#### 決定木の復習

**決定木**は、特徴量に基づく条件分岐（例：\( x_1 \leq 2.5 \) かどうか）でデータを分割し、**葉（リーフ）**と呼ばれる終端ノードでクラスまたは数値を出力する、木構造のモデルです。各ノードでは、分割後の**不純度**（ジニ係数やエントロピーで測る「クラスが混ざっている度合い」）の減少が最大になるような分割を貪欲に選びます。解釈がしやすく、非線形な境界も表現できますが、一本だけでは過学習しやすいため、ランダムフォレストでは多数の木を組み合わせます。

#### バギングとランダム性

- **バギング（Bootstrap Aggregating）**: 訓練データから**ブートストラップサンプル**（復元抽出で同じサイズのデータを複数作成）を用意し、それぞれで決定木を学習する。予測は**多数決（分類）** または**平均（回帰）** で行う。これにより分散が減り、過学習が抑えられやすい。
- **特徴量のランダム選択**: 各ノードの分割時に、**全特徴量ではなく一部の特徴量だけ**を候補にする。木どうしの相関を下げ、さらに性能と頑健性を高める。

この「バギング + 特徴量のランダムサブセット」を組み合わせた手法がランダムフォレストです。

#### ハイパーパラメータの例

- **木の本数（n_estimators）**: 多いほど安定しやすいが、計算コストとメモリが増える。
- **分割時の特徴量数（max_features）**: 分類では \( \sqrt{p} \) 程度、回帰では \( p/3 \) 程度がよく使われる。
- **木の深さ（max_depth）**: 深くしすぎると過学習のリスクが増える。

#### 特徴量重要度

各特徴量が「分割にどれだけ寄与したか」に基づいて**重要度**が計算できます。解釈や特徴量選択の目安に使えます。ただし、相関の高い特徴量どうしでは重要度が分散するため、解釈には注意が必要です。

#### メリット・デメリット

- **メリット**: 過学習に強く、非線形・交互作用を自然に扱える。欠損値の扱い（例：欠損を別方向に分岐）や外れ値にも比較的頑健。学習が並列化しやすい。
- **デメリット**: 解釈は単体の決定木より難しくなる。メモリと計算コストがかかる。外挿（訓練データの範囲外）は苦手。

---

### ブースティング（Boosting）

ブースティングは、**弱い学習器（例：浅い決定木）を順番に追加し、前のモデルの誤りを補正していく**アンサンブル手法です。分類・回帰の両方に使われ、多くのコンペや実務で高い精度が報告されています。

#### 考え方

- 最初のモデルで全体を近似する。
- その**残差（予測と正解の差）** を減らすように次のモデルを学習する。
- これを繰り返し、すべてのモデルの予測を**加算**して最終予測とする（加法的モデル）。

各ステップでは「今のモデルが間違えやすいサンプル」や「残差が大きいサンプル」に重みを置いて学習するため、弱学習器の組み合わせでも強いモデルになります。

#### 代表的なアルゴリズム

- **AdaBoost（Adaptive Boosting）**: 分類が間違ったサンプルの重みを増やし、次の弱学習器でその誤りを減らす。最終予測は弱学習器の加重投票。
- **勾配ブースティング（Gradient Boosting）**: 残差を「損失の勾配」で表し、その勾配にフィットするように次の木を追加する。GBDT（Gradient Boosting Decision Tree）とも呼ばれる。
- **XGBoost**: 勾配ブースティングの効率的な実装。正則化、 early stopping、欠損値の扱いなどが充実。
- **LightGBM**: ヒストグラムベースの分割とリーフワイズ成長で、大規模データでも高速に学習できる。
- **CatBoost**: カテゴリ変数をそのまま扱いやすく、順序付きブースティングで過学習を抑える工夫がある。

#### ハイパーパラメータの例

- **弱学習器の数（n_estimators / num_round）**: 多すぎると過学習のリスク。early stopping で適切な本数を決めることが多い。
- **学習率（learning_rate）**: 小さいと収束は遅いが安定しやすい。木の本数とトレードオフ。
- **木の深さ（max_depth）**: 浅い木（例：3〜6）を多く使うことが多い。
- **正則化**: L1/L2、サンプル・特徴量のサブサンプリングなどで過学習を防ぐ。

#### メリット・デメリット

- **メリット**: 多くのタスクで高い予測精度。特徴量重要度が得られ、解釈の手がかりになる。テーブルデータで特に強い。
- **デメリット**: ハイパーパラメータの影響が大きく、チューニングに手間がかかることがある。学習は逐次的で、ランダムフォレストほど並列化しにくい（実装によっては並列化あり）。

---

### サポートベクターマシン（SVM）

SVMは、**クラス間のマージン（余白）を最大にする超平面**でクラスを分離する分類手法です。カーネルを使うと非線形な境界も表現できます。

#### 線形SVMとマージン

- **ハードマージン**: 訓練データが線形分離可能なとき、**分類を正しく行いながら、超平面と最も近いデータ点（サポートベクター）との距離**を最大化する超平面を求める。
- **ソフトマージン**: 現実のデータでは完全に分離できないことが多いため、**誤分類やマージン内への侵入を許す代わりにペナルティ**を入れた定式化。ペナルティの強さはハイパーパラメータ \( C \) で制御する。\( C \) が大きいと誤分類を許しにくく、小さいとマージンを広く取りやすく過学習が抑えられやすい。

#### 双対問題とサポートベクター

SVMの最適化は**双対問題**として解かれることが多く、解は**サポートベクター**（\( \alpha_i \neq 0 \) となる訓練サンプル）の線形結合で表されます。予測時もサポートベクターとの内積（とカーネル）だけが必要で、高次元でも計算が扱いやすくなります。

#### カーネル関数と非線形SVM

**カーネル法**により、入力 \( x \) を高次元特徴空間に写像せずに、その空間での内積だけを計算できます。これで**非線形な境界**を実現します。

- **RBFカーネル（ガウシアンカーネル）**: \( k(x, x') = \exp(-\gamma \|x - x'\|^2) \)。\( \gamma \) が大きいと境界が複雑になり、小さいと滑らかになる。
- **多項式カーネル**: \( k(x, x') = (x^\top x' + c)^d \)。次数 \( d \) で非線形性を制御。
- **線形カーネル**: そのまま線形SVM。高次元で線形で十分な場合に使う。

#### 回帰への拡張（SVR）

**サポートベクター回帰（SVR）** では、\( \varepsilon \)-チューブ（誤差が \( \varepsilon \) 以内なら損失0）を導入し、チューブの外側の誤差だけをペナルティにして回帰を行います。外れ値の影響を受けにくい場合があります。

#### メリット・デメリット

- **メリット**: 高次元・中規模データで強く、カーネルで非線形に対応。サポートベクターのみがモデルに効くため、解がスパースになりやすい。
- **デメリット**: サンプル数が非常に大きいと計算コストが高い。\( C \) や \( \gamma \) などハイパーパラメータの影響が大きい。確率出力は標準的には含まれない（Platt scaling 等で後から付けることは可能）。

---

### 自己回帰モデル（Autoregressive Model）

自己回帰モデルは、**時系列データ**において「過去の自分自身の値」で現在の値を予測するモデルです。**回帰**の一種ですが、入力が同じ系列のラグ（過去の値）である点が特徴です。

#### ARモデル（Autoregressive Model）

**AR(\( p \))モデル**では、現在の値 \( y_t \) を、過去 \( p \) 時点の値 \( y_{t-1}, y_{t-2}, \ldots, y_{t-p} \) の線形結合で表します。

\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \cdots + \phi_p y_{t-p} + \varepsilon_t
\]

\( c \) は定数項、\( \phi_1, \ldots, \phi_p \) は自己回帰係数、\( \varepsilon_t \) は白色雑音です。**ラグ \( p \)** は、自己相関や偏自己相関、AIC/BICなどで選びます。

#### 移動平均（MA）とARMA

- **MA(\( q \))モデル**: 現在の値を、過去の**誤差（ショック）** \( \varepsilon_{t-1}, \ldots, \varepsilon_{t-q} \) の線形結合で表す。
- **ARMA(\( p, q \))**: ARとMAを組み合わせたモデル。より少ないパラメータで柔軟に時系列を表現できる。

#### ARIMAと季節性

- **ARIMA(\( p, d, q \))**: 非定常な時系列に対して**差分**を \( d \) 回取り、ARMA(\( p, q \))を当てはめる。トレンドを持つデータに広く使われる。
- **SARIMA**: 季節性を明示的に入れたARIMA。季節ラグや季節差分を追加する。

#### 推定と予測

係数 \( \phi, \theta \) の推定には**最尤推定**や**条件付き最小二乗**が使われます。推定したモデルから、**1期先〜多期先の予測**とその**予測区間**を計算できます。

#### 適用の注意点

- **定常性**: AR/ARMAは定常時系列を仮定する。トレンドや季節性がある場合は差分や季節項で定常に近づける。
- **外れ値・構造変化**: 大きな外れ値やレジーム変化があると、係数推定や予測がずれることがある。
- **多変量への拡張**: **VAR（ベクトル自己回帰）** では、複数の時系列を同時にモデル化する。

#### メリット・デメリット

- **メリット**: 数式が明確で解釈しやすく、予測区間が得られる。実装が多くのライブラリで用意されている。
- **デメリット**: 線形・定常を前提としており、複雑な非線形や強い非定常には向かない。深層学習ベースの時系列モデル（LSTM、Transformerなど）は別の選択肢となる。

---

## 分類の評価指標

- **正解率（Accuracy）**: 全体のうち正しく予測した割合。クラスが偏っていると誤解を招くことがある。
- **適合率（Precision）**: 陽性と予測したもののうち、実際に陽性だった割合。
- **再現率（Recall）**: 実際に陽性のもののうち、陽性と予測した割合。
- **F1スコア**: 適合率と再現率の調和平均。クラス不均衡がある場合に有用。

## 回帰の評価指標

- **平均二乗誤差（MSE）**: 予測値と実測値の差の二乗の平均。
- **平均絶対誤差（MAE）**: 予測値と実測値の差の絶対値の平均。
- **決定係数（R²）**: モデルがデータのばらつきをどれだけ説明しているかを表す指標。1に近いほど良い。

## 過学習と対策

教師あり学習では、訓練データに**過度に fitting する過学習（Overfitting）** が起きることがあります。**過学習**とは、訓練誤差は小さいのに、未知データ（テストデータ）での性能が落ちる状態です。モデルが「データの本質的なパターン」だけでなく「そのデータにしかないノイズ」まで覚えてしまったときに起こりがちで、モデルが複雑すぎる場合や訓練データが少ない場合に生じやすくなります。

### 主な対策

- **正則化**: モデルの複雑さにペナルティを加える（リッジ、Lasso、ドロップアウトなど）。
- **交差検証**: データを分割して検証し、汎化性能を評価する。
- **データ拡張**: 訓練データを人工的に増やす（画像の回転・切り抜きなど）。
- **早期終了**: 検証誤差が悪化し始めたら学習を止める（ニューラルネットなど）。
- **十分なデータ**: 可能であればデータ量を増やす。

### アルゴリズム選びの目安

| 目的 | 候補 |
|------|------|
| 解釈を重視する回帰 | 線形回帰、リッジ・Lasso |
| 解釈を重視する分類 | ロジスティック回帰 |
| 高精度・テーブルデータ | ランダムフォレスト、ブースティング（XGBoost など） |
| 中規模・高次元・非線形な境界 | SVM（カーネル） |
| 時系列の予測 | 自己回帰モデル（ARIMA など） |

※ 実際にはデータ規模・特徴量の性質・計算資源に応じて選択し、交差検証などで比較するとよいです。

## まとめ

本記事の要点をまとめます。

- 教師あり学習は、**入力（特徴量）と正解（ラベル）のペア**から予測モデルを学習します。**分類**は離散的なクラスを、**回帰**は連続的な数値を予測するタスクです。
- **線形回帰**は最小二乗法（OLS）で係数を推定し、リッジ・Lasso・Elastic Net で正則化できます。解釈が容易で、回帰の基礎となるモデルです。
- **ロジスティック回帰**はシグモイド関数で確率を出力する二値（多クラス）分類モデルです。係数の符号や大きさから「どの特徴が効いているか」が解釈しやすく、実務で広く使われます。
- **ランダムフォレスト**はバギングと特徴量のランダム選択で多数の決定木を組み合わせ、多数決（分類）または平均（回帰）で予測します。頑健で非線形・交互作用にも自然に対応できます。
- **ブースティング**は弱学習器を順次追加し、残差を減らしていく加法的なモデルです。勾配ブースティング・XGBoost・LightGBM・CatBoostなどが広く使われ、テーブルデータで高い精度が出やすいです。
- **SVM**はマージンを最大化する超平面で分類し、カーネルで非線形な境界に対応します。SVRで回帰にも拡張でき、高次元・中規模データで強みがあります。
- **自己回帰モデル（AR/ARIMA）**は時系列の過去の値で現在を予測するモデルです。定常性の仮定の下で係数推定と予測区間が得られ、需要予測や経済時系列などに使われます。
- **過学習**（訓練データに過度に fitting し、未知データで性能が落ちる状態）を防ぐために、正則化・交差検証・early stopping・十分なデータ量などを活用します。

**モデルの選択・評価**（データの扱い、予測誤差・正解率・適合率・再現率・F値・ROC曲線とAUC、AIC・BIC）の詳細は、別記事「[モデルの選択・評価](/posts/ml-overview-05-model-selection-evaluation)」を参照してください。次の記事では、**教師なし学習**の階層なし・階層ありクラスタリング、主成分分析、協調フィルタリング、トピックモデルについて解説します。
